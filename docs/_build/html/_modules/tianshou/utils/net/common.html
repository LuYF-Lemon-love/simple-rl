<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>tianshou.utils.net.common &mdash; Tianshou 0.5.1 documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/style.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/documentation_options.js?v=b9afe91b"></script>
        <script src="../../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="https://cdn.jsdelivr.net/npm/vega@5.20.2"></script>
        <script src="https://cdn.jsdelivr.net/npm/vega-lite@5.1.0"></script>
        <script src="https://cdn.jsdelivr.net/npm/vega-embed@6.17.0"></script>
        <script src="../../../../_static/js/copybutton.js?v=7db002fe"></script>
        <script src="../../../../_static/js/benchmark.js?v=1091b9f3"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Tianshou
              <img src="../../../../_static/tianshou-logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.5.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/get_started.html">Get Started with Jupyter Notebook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/dqn.html">Deep Q Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/concepts.html">Basic concepts in Tianshou</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/batch.html">Understand Batch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/tictactoe.html">Multi-Agent RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/logger.html">Logging Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/benchmark.html">Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/cheatsheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Docs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/tianshou.data.html">tianshou.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/tianshou.env.html">tianshou.env</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/tianshou.policy.html">tianshou.policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/tianshou.trainer.html">tianshou.trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/tianshou.exploration.html">tianshou.exploration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/tianshou.utils.html">tianshou.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../contributing.html">Contributing to Tianshou</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../contributor.html">Contributor</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Tianshou</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">tianshou.utils.net.common</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for tianshou.utils.net.common</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="n">no_type_check</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="kn">from</span> <span class="nn">tianshou.data.batch</span> <span class="kn">import</span> <span class="n">Batch</span>
<span class="kn">from</span> <span class="nn">tianshou.data.types</span> <span class="kn">import</span> <span class="n">RecurrentStateBatch</span>

<span class="n">ModuleType</span> <span class="o">=</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span>
<span class="n">ArgsType</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
                 <span class="n">Sequence</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]</span>


<div class="viewcode-block" id="miniblock">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.miniblock">[docs]</a>
<span class="k">def</span> <span class="nf">miniblock</span><span class="p">(</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">act_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">linear_layer</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a miniblock with given input/output-size, norm layer and \</span>
<span class="sd">    activation.&quot;&quot;&quot;</span>
    <span class="n">layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">linear_layer</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">norm_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">norm_args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">norm_layer</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="o">*</span><span class="n">norm_args</span><span class="p">)]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">norm_args</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">norm_layer</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="o">**</span><span class="n">norm_args</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">norm_layer</span><span class="p">(</span><span class="n">output_size</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">activation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">act_args</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">activation</span><span class="p">(</span><span class="o">*</span><span class="n">act_args</span><span class="p">)]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">act_args</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">activation</span><span class="p">(</span><span class="o">**</span><span class="n">act_args</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">activation</span><span class="p">()]</span>
    <span class="k">return</span> <span class="n">layers</span></div>



<div class="viewcode-block" id="MLP">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.MLP">[docs]</a>
<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple MLP backbone.</span>

<span class="sd">    Create a MLP of size input_dim * hidden_sizes[0] * hidden_sizes[1] * ...</span>
<span class="sd">    * hidden_sizes[-1] * output_dim</span>

<span class="sd">    :param int input_dim: dimension of the input vector.</span>
<span class="sd">    :param int output_dim: dimension of the output vector. If set to 0, there</span>
<span class="sd">        is no final linear layer.</span>
<span class="sd">    :param hidden_sizes: shape of MLP passed in as a list, not including</span>
<span class="sd">        input_dim and output_dim.</span>
<span class="sd">    :param norm_layer: use which normalization before activation, e.g.,</span>
<span class="sd">        ``nn.LayerNorm`` and ``nn.BatchNorm1d``. Default to no normalization.</span>
<span class="sd">        You can also pass a list of normalization modules with the same length</span>
<span class="sd">        of hidden_sizes, to use different normalization module in different</span>
<span class="sd">        layers. Default to no normalization.</span>
<span class="sd">    :param activation: which activation to use after each layer, can be both</span>
<span class="sd">        the same activation for all layers if passed in nn.Module, or different</span>
<span class="sd">        activation for different Modules if passed in a list. Default to</span>
<span class="sd">        nn.ReLU.</span>
<span class="sd">    :param device: which device to create this model on. Default to None.</span>
<span class="sd">    :param linear_layer: use this module as linear layer. Default to nn.Linear.</span>
<span class="sd">    :param bool flatten_input: whether to flatten input data. Default to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">hidden_sizes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ArgsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">]]]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
        <span class="n">act_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ArgsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">linear_layer</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span>
        <span class="n">flatten_input</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="k">if</span> <span class="n">norm_layer</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">norm_layer</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">norm_layer</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span>
                <span class="n">norm_layer_list</span> <span class="o">=</span> <span class="n">norm_layer</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">norm_args</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">norm_args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span>
                    <span class="n">norm_args_list</span> <span class="o">=</span> <span class="n">norm_args</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">norm_args_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">norm_args</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">))]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">norm_layer_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">norm_layer</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">))]</span>
                <span class="n">norm_args_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">norm_args</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">))]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">norm_layer_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span>
            <span class="n">norm_args_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">activation</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span>
                <span class="n">activation_list</span> <span class="o">=</span> <span class="n">activation</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">act_args</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">act_args</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span>
                    <span class="n">act_args_list</span> <span class="o">=</span> <span class="n">act_args</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">act_args_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">act_args</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">))]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">activation_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">activation</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">))]</span>
                <span class="n">act_args_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">act_args</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">))]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">activation_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span>
            <span class="n">act_args_list</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span>
        <span class="n">hidden_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_dim</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">norm</span><span class="p">,</span> <span class="n">norm_args</span><span class="p">,</span> <span class="n">activ</span><span class="p">,</span> <span class="n">act_args</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">hidden_sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">hidden_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
            <span class="n">norm_layer_list</span><span class="p">,</span>
            <span class="n">norm_args_list</span><span class="p">,</span>
            <span class="n">activation_list</span><span class="p">,</span>
            <span class="n">act_args_list</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">model</span> <span class="o">+=</span> <span class="n">miniblock</span><span class="p">(</span>
                <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">norm</span><span class="p">,</span> <span class="n">norm_args</span><span class="p">,</span> <span class="n">activ</span><span class="p">,</span> <span class="n">act_args</span><span class="p">,</span> <span class="n">linear_layer</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">output_dim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">+=</span> <span class="p">[</span><span class="n">linear_layer</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_dim</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span> <span class="ow">or</span> <span class="n">hidden_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten_input</span> <span class="o">=</span> <span class="n">flatten_input</span>

<div class="viewcode-block" id="MLP.forward">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.MLP.forward">[docs]</a>
    <span class="nd">@no_type_check</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten_input</span><span class="p">:</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="NetBase">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.NetBase">[docs]</a>
<span class="k">class</span> <span class="nc">NetBase</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Interface for NNs used in policies.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="NetBase.forward">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.NetBase.forward">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">pass</span></div>
</div>



<div class="viewcode-block" id="Net">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.Net">[docs]</a>
<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">NetBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wrapper of MLP to support more specific DRL usage.</span>

<span class="sd">    For advanced usage (how to customize the network), please refer to</span>
<span class="sd">    :ref:`build_the_network`.</span>

<span class="sd">    :param state_shape: int or a sequence of int of the shape of state.</span>
<span class="sd">    :param action_shape: int or a sequence of int of the shape of action.</span>
<span class="sd">    :param hidden_sizes: shape of MLP passed in as a list.</span>
<span class="sd">    :param norm_layer: use which normalization before activation, e.g.,</span>
<span class="sd">        ``nn.LayerNorm`` and ``nn.BatchNorm1d``. Default to no normalization.</span>
<span class="sd">        You can also pass a list of normalization modules with the same length</span>
<span class="sd">        of hidden_sizes, to use different normalization module in different</span>
<span class="sd">        layers. Default to no normalization.</span>
<span class="sd">    :param activation: which activation to use after each layer, can be both</span>
<span class="sd">        the same activation for all layers if passed in nn.Module, or different</span>
<span class="sd">        activation for different Modules if passed in a list. Default to</span>
<span class="sd">        nn.ReLU.</span>
<span class="sd">    :param device: specify the device when the network actually runs. Default</span>
<span class="sd">        to &quot;cpu&quot;.</span>
<span class="sd">    :param bool softmax: whether to apply a softmax layer over the last layer&#39;s</span>
<span class="sd">        output.</span>
<span class="sd">    :param bool concat: whether the input shape is concatenated by state_shape</span>
<span class="sd">        and action_shape. If it is True, ``action_shape`` is not the output</span>
<span class="sd">        shape, but affects the input shape only.</span>
<span class="sd">    :param int num_atoms: in order to expand to the net of distributional RL.</span>
<span class="sd">        Default to 1 (not use).</span>
<span class="sd">    :param bool dueling_param: whether to use dueling network to calculate Q</span>
<span class="sd">        values (for Dueling DQN). If you want to use dueling option, you should</span>
<span class="sd">        pass a tuple of two dict (first for Q and second for V) stating</span>
<span class="sd">        self-defined arguments as stated in</span>
<span class="sd">        class:`~tianshou.utils.net.common.MLP`. Default to None.</span>
<span class="sd">    :param linear_layer: use this module as linear layer. Default to nn.Linear.</span>

<span class="sd">    .. seealso::</span>

<span class="sd">        Please refer to :class:`~tianshou.utils.net.common.MLP` for more</span>
<span class="sd">        detailed explanation on the usage of activation, norm_layer, etc.</span>

<span class="sd">        You can also refer to :class:`~tianshou.utils.net.continuous.Actor`,</span>
<span class="sd">        :class:`~tianshou.utils.net.continuous.Critic`, etc, to see how it&#39;s</span>
<span class="sd">        suggested be used.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">action_shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">hidden_sizes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ArgsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">]]]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
        <span class="n">act_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ArgsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">softmax</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">concat</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_atoms</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">dueling_param</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">linear_layer</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">softmax</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_atoms</span> <span class="o">=</span> <span class="n">num_atoms</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MLP</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MLP</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">state_shape</span><span class="p">))</span>
        <span class="n">action_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">action_shape</span><span class="p">))</span> <span class="o">*</span> <span class="n">num_atoms</span>
        <span class="k">if</span> <span class="n">concat</span><span class="p">:</span>
            <span class="n">input_dim</span> <span class="o">+=</span> <span class="n">action_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_dueling</span> <span class="o">=</span> <span class="n">dueling_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">output_dim</span> <span class="o">=</span> <span class="n">action_dim</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dueling</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">concat</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="p">,</span>
            <span class="n">output_dim</span><span class="p">,</span>
            <span class="n">hidden_sizes</span><span class="p">,</span>
            <span class="n">norm_layer</span><span class="p">,</span>
            <span class="n">norm_args</span><span class="p">,</span>
            <span class="n">activation</span><span class="p">,</span>
            <span class="n">act_args</span><span class="p">,</span>
            <span class="n">device</span><span class="p">,</span>
            <span class="n">linear_layer</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dueling</span><span class="p">:</span>  <span class="c1"># dueling DQN</span>
            <span class="k">assert</span> <span class="n">dueling_param</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">kwargs_update</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;input_dim&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span>
                <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="c1"># Important: don&#39;t change the original dict (e.g., don&#39;t use .update())</span>
            <span class="n">q_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">dueling_param</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs_update</span><span class="p">}</span>
            <span class="n">v_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">dueling_param</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs_update</span><span class="p">}</span>

            <span class="n">q_kwargs</span><span class="p">[</span><span class="s2">&quot;output_dim&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">concat</span> <span class="k">else</span> <span class="n">action_dim</span>
            <span class="n">v_kwargs</span><span class="p">[</span><span class="s2">&quot;output_dim&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">concat</span> <span class="k">else</span> <span class="n">num_atoms</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="o">**</span><span class="n">q_kwargs</span><span class="p">),</span> <span class="n">MLP</span><span class="p">(</span><span class="o">**</span><span class="n">v_kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">output_dim</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">output_dim</span>

<div class="viewcode-block" id="Net.forward">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.Net.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Mapping: obs -&gt; flatten (inside MLP)-&gt; logits.</span>

<span class="sd">        :param obs:</span>
<span class="sd">        :param state: unused and returned as is</span>
<span class="sd">        :param kwargs: unused</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_dueling</span><span class="p">:</span>  <span class="c1"># Dueling DQN</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">q</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_atoms</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_atoms</span><span class="p">)</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_atoms</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">q</span> <span class="o">-</span> <span class="n">q</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_atoms</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_atoms</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">state</span></div>
</div>



<div class="viewcode-block" id="Recurrent">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.Recurrent">[docs]</a>
<span class="k">class</span> <span class="nc">Recurrent</span><span class="p">(</span><span class="n">NetBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple Recurrent network based on LSTM.</span>

<span class="sd">    For advanced usage (how to customize the network), please refer to</span>
<span class="sd">    :ref:`build_the_network`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">layer_num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">state_shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">action_shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
        <span class="n">hidden_layer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span>
            <span class="n">input_size</span><span class="o">=</span><span class="n">hidden_layer_size</span><span class="p">,</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_layer_size</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">layer_num</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">state_shape</span><span class="p">)),</span> <span class="n">hidden_layer_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_layer_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">action_shape</span><span class="p">)))</span>

<div class="viewcode-block" id="Recurrent.forward">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.Recurrent.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">RecurrentStateBatch</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Mapping: obs -&gt; flatten -&gt; logits.</span>

<span class="sd">        In the evaluation mode, `obs` should be with shape ``[bsz, dim]``; in the</span>
<span class="sd">        training mode, `obs` should be with shape ``[bsz, len, dim]``. See the code</span>
<span class="sd">        and comment for more detail.</span>

<span class="sd">        :param obs:</span>
<span class="sd">        :param state: either None or a dict with keys &#39;hidden&#39; and &#39;cell&#39;</span>
<span class="sd">        :param kwargs: unused</span>
<span class="sd">        :return: predicted action, next state as dict with keys &#39;hidden&#39; and &#39;cell&#39;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Note: the original type of state is Batch but it might also be a dict</span>
        <span class="c1"># If it is a Batch, .issubset(state) will not work. However,</span>
        <span class="c1"># issubset(state.keys()) always works</span>
        <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">{</span><span class="s2">&quot;hidden&quot;</span><span class="p">,</span> <span class="s2">&quot;cell&quot;</span><span class="p">}</span><span class="o">.</span><span class="n">issubset</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected to find keys &#39;hidden&#39; and &#39;cell&#39; &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but instead found </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="c1"># obs [bsz, len, dim] (training) or [bsz, dim] (evaluation)</span>
        <span class="c1"># In short, the tensor&#39;s shape in training phase is longer than which</span>
        <span class="c1"># in evaluation phase.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">obs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">flatten_parameters</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">obs</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nn</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># we store the stack data in [bsz, len, ...] format</span>
            <span class="c1"># but pytorch rnn needs [len, bsz, ...]</span>
            <span class="n">obs</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nn</span><span class="p">(</span>
                <span class="n">obs</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;hidden&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
                    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;cell&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">obs</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># please ensure the first dim is batch size: [bsz, len, ...]</span>
        <span class="k">return</span> <span class="n">obs</span><span class="p">,</span> <span class="p">{</span>
            <span class="s2">&quot;hidden&quot;</span><span class="p">:</span> <span class="n">hidden</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span>
            <span class="s2">&quot;cell&quot;</span><span class="p">:</span> <span class="n">cell</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span>
        <span class="p">}</span></div>
</div>



<div class="viewcode-block" id="ActorCritic">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.ActorCritic">[docs]</a>
<span class="k">class</span> <span class="nc">ActorCritic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An actor-critic network for parsing parameters.</span>

<span class="sd">    Using ``actor_critic.parameters()`` instead of set.union or list+list to avoid</span>
<span class="sd">    issue #449.</span>

<span class="sd">    :param nn.Module actor: the actor network.</span>
<span class="sd">    :param nn.Module critic: the critic network.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actor</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">critic</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actor</span> <span class="o">=</span> <span class="n">actor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">critic</span> <span class="o">=</span> <span class="n">critic</span></div>



<div class="viewcode-block" id="DataParallelNet">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.DataParallelNet">[docs]</a>
<span class="k">class</span> <span class="nc">DataParallelNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;DataParallel wrapper for training agent with multi-GPU.</span>

<span class="sd">    This class does only the conversion of input data type, from numpy array to torch&#39;s</span>
<span class="sd">    Tensor. If the input is a nested dictionary, the user should create a similar class</span>
<span class="sd">    to do the same thing.</span>

<span class="sd">    :param nn.Module net: the network to be distributed in different GPUs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>

<div class="viewcode-block" id="DataParallelNet.forward">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.DataParallelNet.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">obs</span><span class="o">=</span><span class="n">obs</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="EnsembleLinear">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.EnsembleLinear">[docs]</a>
<span class="k">class</span> <span class="nc">EnsembleLinear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear Layer of Ensemble network.</span>

<span class="sd">    :param int ensemble_size: Number of subnets in the ensemble.</span>
<span class="sd">    :param int in_feature: dimension of the input vector.</span>
<span class="sd">    :param int out_feature: dimension of the output vector.</span>
<span class="sd">    :param bool bias: whether to include an additive bias, default to be True.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ensemble_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">in_feature</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_feature</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># To be consistent with PyTorch default initializer</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">in_feature</span><span class="p">)</span>
        <span class="n">weight_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">ensemble_size</span><span class="p">,</span> <span class="n">in_feature</span><span class="p">,</span> <span class="n">out_feature</span><span class="p">))</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k</span> <span class="o">-</span> <span class="n">k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">weight_data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bias_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="n">bias_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">ensemble_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_feature</span><span class="p">))</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k</span> <span class="o">-</span> <span class="n">k</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">bias_data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<div class="viewcode-block" id="EnsembleLinear.forward">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.EnsembleLinear.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_weights</span>
        <span class="k">return</span> <span class="n">x</span></div>
</div>



<div class="viewcode-block" id="BranchingNet">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.BranchingNet">[docs]</a>
<span class="k">class</span> <span class="nc">BranchingNet</span><span class="p">(</span><span class="n">NetBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Branching dual Q network.</span>

<span class="sd">    Network for the BranchingDQNPolicy, it uses a common network module, a value module</span>
<span class="sd">    and action &quot;branches&quot; one for each dimension.It allows for a linear scaling</span>
<span class="sd">    of Q-value the output w.r.t. the number of dimensions in the action space.</span>
<span class="sd">    For more info please refer to: arXiv:1711.08946.</span>
<span class="sd">    :param state_shape: int or a sequence of int of the shape of state.</span>
<span class="sd">    :param action_shape: int or a sequence of int of the shape of action.</span>
<span class="sd">    :param action_peer_branch: int or a sequence of int of the number of actions in</span>
<span class="sd">    each dimension.</span>
<span class="sd">    :param common_hidden_sizes: shape of the common MLP network passed in as a list.</span>
<span class="sd">    :param value_hidden_sizes: shape of the value MLP network passed in as a list.</span>
<span class="sd">    :param action_hidden_sizes: shape of the action MLP network passed in as a list.</span>
<span class="sd">    :param norm_layer: use which normalization before activation, e.g.,</span>
<span class="sd">    ``nn.LayerNorm`` and ``nn.BatchNorm1d``. Default to no normalization.</span>
<span class="sd">    You can also pass a list of normalization modules with the same length</span>
<span class="sd">    of hidden_sizes, to use different normalization module in different</span>
<span class="sd">    layers. Default to no normalization.</span>
<span class="sd">    :param activation: which activation to use after each layer, can be both</span>
<span class="sd">    the same activation for all layers if passed in nn.Module, or different</span>
<span class="sd">    activation for different Modules if passed in a list. Default to</span>
<span class="sd">    nn.ReLU.</span>
<span class="sd">    :param device: specify the device when the network actually runs. Default</span>
<span class="sd">    to &quot;cpu&quot;.</span>
<span class="sd">    :param bool softmax: whether to apply a softmax layer over the last layer&#39;s</span>
<span class="sd">    output.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">state_shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">num_branches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">action_per_branch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">common_hidden_sizes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">value_hidden_sizes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_hidden_sizes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">norm_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ArgsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ModuleType</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
        <span class="n">act_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ArgsType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">common_hidden_sizes</span> <span class="o">=</span> <span class="n">common_hidden_sizes</span> <span class="ow">or</span> <span class="p">[]</span>
        <span class="n">value_hidden_sizes</span> <span class="o">=</span> <span class="n">value_hidden_sizes</span> <span class="ow">or</span> <span class="p">[]</span>
        <span class="n">action_hidden_sizes</span> <span class="o">=</span> <span class="n">action_hidden_sizes</span> <span class="ow">or</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_branches</span> <span class="o">=</span> <span class="n">num_branches</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_per_branch</span> <span class="o">=</span> <span class="n">action_per_branch</span>
        <span class="c1"># common network</span>
        <span class="n">common_input_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">state_shape</span><span class="p">))</span>
        <span class="n">common_output_dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">common</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">common_input_dim</span><span class="p">,</span>
            <span class="n">common_output_dim</span><span class="p">,</span>
            <span class="n">common_hidden_sizes</span><span class="p">,</span>
            <span class="n">norm_layer</span><span class="p">,</span>
            <span class="n">norm_args</span><span class="p">,</span>
            <span class="n">activation</span><span class="p">,</span>
            <span class="n">act_args</span><span class="p">,</span>
            <span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># value network</span>
        <span class="n">value_input_dim</span> <span class="o">=</span> <span class="n">common_hidden_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">value_output_dim</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">value_input_dim</span><span class="p">,</span>
            <span class="n">value_output_dim</span><span class="p">,</span>
            <span class="n">value_hidden_sizes</span><span class="p">,</span>
            <span class="n">norm_layer</span><span class="p">,</span>
            <span class="n">norm_args</span><span class="p">,</span>
            <span class="n">activation</span><span class="p">,</span>
            <span class="n">act_args</span><span class="p">,</span>
            <span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># action branching network</span>
        <span class="n">action_input_dim</span> <span class="o">=</span> <span class="n">common_hidden_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">action_output_dim</span> <span class="o">=</span> <span class="n">action_per_branch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">branches</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">MLP</span><span class="p">(</span>
                    <span class="n">action_input_dim</span><span class="p">,</span>
                    <span class="n">action_output_dim</span><span class="p">,</span>
                    <span class="n">action_hidden_sizes</span><span class="p">,</span>
                    <span class="n">norm_layer</span><span class="p">,</span>
                    <span class="n">norm_args</span><span class="p">,</span>
                    <span class="n">activation</span><span class="p">,</span>
                    <span class="n">act_args</span><span class="p">,</span>
                    <span class="n">device</span><span class="p">,</span>
                <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_branches</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BranchingNet.forward">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.BranchingNet.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Mapping: obs -&gt; model -&gt; logits.&quot;&quot;&quot;</span>
        <span class="n">common_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">common</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="n">value_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">(</span><span class="n">common_out</span><span class="p">)</span>
        <span class="n">value_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">value_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">action_out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">branches</span><span class="p">:</span>
            <span class="n">action_out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">b</span><span class="p">(</span><span class="n">common_out</span><span class="p">))</span>
        <span class="n">action_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">action_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">action_scores</span> <span class="o">=</span> <span class="n">action_scores</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">action_scores</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">value_out</span> <span class="o">+</span> <span class="n">action_scores</span>
        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">state</span></div>
</div>



<div class="viewcode-block" id="get_dict_state_decorator">
<a class="viewcode-back" href="../../../../api/tianshou.utils.html#tianshou.utils.net.common.get_dict_state_decorator">[docs]</a>
<span class="k">def</span> <span class="nf">get_dict_state_decorator</span><span class="p">(</span>
    <span class="n">state_shape</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]],</span> <span class="n">keys</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A helper function to make Net or equivalent classes (e.g. Actor, Critic) \</span>
<span class="sd">    applicable to dict state.</span>

<span class="sd">    The first return item, ``decorator_fn``, will alter the implementation of forward</span>
<span class="sd">    function of the given class by preprocessing the observation. The preprocessing is</span>
<span class="sd">    basically flatten the observation and concatenate them based on the ``keys`` order.</span>
<span class="sd">    The batch dimension is preserved if presented. The result observation shape will</span>
<span class="sd">    be equal to ``new_state_shape``, the second return item.</span>

<span class="sd">    :param state_shape: A dictionary indicating each state&#39;s shape</span>
<span class="sd">    :param keys: A list of state&#39;s keys. The flatten observation will be according to \</span>
<span class="sd">    this list order.</span>
<span class="sd">    :returns: a 2-items tuple ``decorator_fn`` and ``new_state_shape``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="n">state_shape</span>
    <span class="n">flat_state_shapes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
        <span class="n">flat_state_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">state_shape</span><span class="p">[</span><span class="n">k</span><span class="p">])))</span>
    <span class="n">new_state_shape</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">flat_state_shapes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">preprocess_obs</span><span class="p">(</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Batch</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">Batch</span><span class="p">)</span> <span class="ow">and</span> <span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">in</span> <span class="n">obs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">original_shape</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">==</span> <span class="n">obs</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="c1"># No batch dim</span>
                <span class="n">new_obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">obs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
                <span class="c1"># new_obs = torch.Tensor([obs[k] for k in keys]).reshape(1, -1)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">bsz</span> <span class="o">=</span> <span class="n">obs</span><span class="p">[</span><span class="n">keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">new_obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_obs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_obs</span>

    <span class="nd">@no_type_check</span>
    <span class="k">def</span> <span class="nf">decorator_fn</span><span class="p">(</span><span class="n">net_class</span><span class="p">):</span>

        <span class="k">class</span> <span class="nc">new_net_class</span><span class="p">(</span><span class="n">net_class</span><span class="p">):</span>

            <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">preprocess_obs</span><span class="p">(</span><span class="n">obs</span><span class="p">),</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">new_net_class</span>

    <span class="k">return</span> <span class="n">decorator_fn</span><span class="p">,</span> <span class="n">new_state_shape</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Tianshou contributors..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>