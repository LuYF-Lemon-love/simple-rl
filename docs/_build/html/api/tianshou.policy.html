<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>tianshou.policy &mdash; Tianshou 0.5.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/style.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/documentation_options.js?v=b9afe91b"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="https://cdn.jsdelivr.net/npm/vega@5.20.2"></script>
        <script src="https://cdn.jsdelivr.net/npm/vega-lite@5.1.0"></script>
        <script src="https://cdn.jsdelivr.net/npm/vega-embed@6.17.0"></script>
        <script src="../_static/js/copybutton.js?v=7db002fe"></script>
        <script src="../_static/js/benchmark.js?v=1091b9f3"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="tianshou.trainer" href="tianshou.trainer.html" />
    <link rel="prev" title="tianshou.env" href="tianshou.env.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Tianshou
              <img src="../_static/tianshou-logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.5.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/get_started.html">Get Started with Jupyter Notebook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/dqn.html">Deep Q Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/concepts.html">Basic concepts in Tianshou</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/batch.html">Understand Batch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/tictactoe.html">Multi-Agent RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/logger.html">Logging Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/benchmark.html">Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/cheatsheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Docs</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="tianshou.data.html">tianshou.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="tianshou.env.html">tianshou.env</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">tianshou.policy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#base">Base</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.BasePolicy"><code class="docutils literal notranslate"><span class="pre">BasePolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.set_agent_id"><code class="docutils literal notranslate"><span class="pre">BasePolicy.set_agent_id()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.exploration_noise"><code class="docutils literal notranslate"><span class="pre">BasePolicy.exploration_noise()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.soft_update"><code class="docutils literal notranslate"><span class="pre">BasePolicy.soft_update()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.forward"><code class="docutils literal notranslate"><span class="pre">BasePolicy.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.map_action"><code class="docutils literal notranslate"><span class="pre">BasePolicy.map_action()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.map_action_inverse"><code class="docutils literal notranslate"><span class="pre">BasePolicy.map_action_inverse()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.process_fn"><code class="docutils literal notranslate"><span class="pre">BasePolicy.process_fn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.learn"><code class="docutils literal notranslate"><span class="pre">BasePolicy.learn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.post_process_fn"><code class="docutils literal notranslate"><span class="pre">BasePolicy.post_process_fn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.update"><code class="docutils literal notranslate"><span class="pre">BasePolicy.update()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.value_mask"><code class="docutils literal notranslate"><span class="pre">BasePolicy.value_mask()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.compute_episodic_return"><code class="docutils literal notranslate"><span class="pre">BasePolicy.compute_episodic_return()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BasePolicy.compute_nstep_return"><code class="docutils literal notranslate"><span class="pre">BasePolicy.compute_nstep_return()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.RandomPolicy"><code class="docutils literal notranslate"><span class="pre">RandomPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.RandomPolicy.forward"><code class="docutils literal notranslate"><span class="pre">RandomPolicy.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.RandomPolicy.learn"><code class="docutils literal notranslate"><span class="pre">RandomPolicy.learn()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-free">Model-free</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dqn-family">DQN Family</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.DQNPolicy"><code class="docutils literal notranslate"><span class="pre">DQNPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BranchingDQNPolicy"><code class="docutils literal notranslate"><span class="pre">BranchingDQNPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.C51Policy"><code class="docutils literal notranslate"><span class="pre">C51Policy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.RainbowPolicy"><code class="docutils literal notranslate"><span class="pre">RainbowPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.QRDQNPolicy"><code class="docutils literal notranslate"><span class="pre">QRDQNPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.IQNPolicy"><code class="docutils literal notranslate"><span class="pre">IQNPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.FQFPolicy"><code class="docutils literal notranslate"><span class="pre">FQFPolicy</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#on-policy">On-policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.PGPolicy"><code class="docutils literal notranslate"><span class="pre">PGPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.NPGPolicy"><code class="docutils literal notranslate"><span class="pre">NPGPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.A2CPolicy"><code class="docutils literal notranslate"><span class="pre">A2CPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.TRPOPolicy"><code class="docutils literal notranslate"><span class="pre">TRPOPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.PPOPolicy"><code class="docutils literal notranslate"><span class="pre">PPOPolicy</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#off-policy">Off-policy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.DDPGPolicy"><code class="docutils literal notranslate"><span class="pre">DDPGPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.TD3Policy"><code class="docutils literal notranslate"><span class="pre">TD3Policy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.SACPolicy"><code class="docutils literal notranslate"><span class="pre">SACPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.REDQPolicy"><code class="docutils literal notranslate"><span class="pre">REDQPolicy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.DiscreteSACPolicy"><code class="docutils literal notranslate"><span class="pre">DiscreteSACPolicy</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#imitation">Imitation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.ImitationPolicy"><code class="docutils literal notranslate"><span class="pre">ImitationPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.ImitationPolicy.forward"><code class="docutils literal notranslate"><span class="pre">ImitationPolicy.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.ImitationPolicy.learn"><code class="docutils literal notranslate"><span class="pre">ImitationPolicy.learn()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.BCQPolicy"><code class="docutils literal notranslate"><span class="pre">BCQPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BCQPolicy.train"><code class="docutils literal notranslate"><span class="pre">BCQPolicy.train()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BCQPolicy.forward"><code class="docutils literal notranslate"><span class="pre">BCQPolicy.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BCQPolicy.sync_weight"><code class="docutils literal notranslate"><span class="pre">BCQPolicy.sync_weight()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.BCQPolicy.learn"><code class="docutils literal notranslate"><span class="pre">BCQPolicy.learn()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.CQLPolicy"><code class="docutils literal notranslate"><span class="pre">CQLPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.CQLPolicy.train"><code class="docutils literal notranslate"><span class="pre">CQLPolicy.train()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.CQLPolicy.sync_weight"><code class="docutils literal notranslate"><span class="pre">CQLPolicy.sync_weight()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.CQLPolicy.actor_pred"><code class="docutils literal notranslate"><span class="pre">CQLPolicy.actor_pred()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.CQLPolicy.calc_actor_loss"><code class="docutils literal notranslate"><span class="pre">CQLPolicy.calc_actor_loss()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.CQLPolicy.calc_pi_values"><code class="docutils literal notranslate"><span class="pre">CQLPolicy.calc_pi_values()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.CQLPolicy.calc_random_values"><code class="docutils literal notranslate"><span class="pre">CQLPolicy.calc_random_values()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.CQLPolicy.process_fn"><code class="docutils literal notranslate"><span class="pre">CQLPolicy.process_fn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.CQLPolicy.learn"><code class="docutils literal notranslate"><span class="pre">CQLPolicy.learn()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.TD3BCPolicy"><code class="docutils literal notranslate"><span class="pre">TD3BCPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.TD3BCPolicy.learn"><code class="docutils literal notranslate"><span class="pre">TD3BCPolicy.learn()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.DiscreteBCQPolicy"><code class="docutils literal notranslate"><span class="pre">DiscreteBCQPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.DiscreteBCQPolicy.train"><code class="docutils literal notranslate"><span class="pre">DiscreteBCQPolicy.train()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.DiscreteBCQPolicy.forward"><code class="docutils literal notranslate"><span class="pre">DiscreteBCQPolicy.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.DiscreteBCQPolicy.learn"><code class="docutils literal notranslate"><span class="pre">DiscreteBCQPolicy.learn()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.DiscreteCQLPolicy"><code class="docutils literal notranslate"><span class="pre">DiscreteCQLPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.DiscreteCQLPolicy.learn"><code class="docutils literal notranslate"><span class="pre">DiscreteCQLPolicy.learn()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.DiscreteCRRPolicy"><code class="docutils literal notranslate"><span class="pre">DiscreteCRRPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.DiscreteCRRPolicy.sync_weight"><code class="docutils literal notranslate"><span class="pre">DiscreteCRRPolicy.sync_weight()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.DiscreteCRRPolicy.learn"><code class="docutils literal notranslate"><span class="pre">DiscreteCRRPolicy.learn()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.GAILPolicy"><code class="docutils literal notranslate"><span class="pre">GAILPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.GAILPolicy.process_fn"><code class="docutils literal notranslate"><span class="pre">GAILPolicy.process_fn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.GAILPolicy.disc"><code class="docutils literal notranslate"><span class="pre">GAILPolicy.disc()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.GAILPolicy.learn"><code class="docutils literal notranslate"><span class="pre">GAILPolicy.learn()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-based">Model-based</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.PSRLPolicy"><code class="docutils literal notranslate"><span class="pre">PSRLPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.PSRLPolicy.forward"><code class="docutils literal notranslate"><span class="pre">PSRLPolicy.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.PSRLPolicy.learn"><code class="docutils literal notranslate"><span class="pre">PSRLPolicy.learn()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.ICMPolicy"><code class="docutils literal notranslate"><span class="pre">ICMPolicy</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.ICMPolicy.train"><code class="docutils literal notranslate"><span class="pre">ICMPolicy.train()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.ICMPolicy.forward"><code class="docutils literal notranslate"><span class="pre">ICMPolicy.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.ICMPolicy.exploration_noise"><code class="docutils literal notranslate"><span class="pre">ICMPolicy.exploration_noise()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.ICMPolicy.set_eps"><code class="docutils literal notranslate"><span class="pre">ICMPolicy.set_eps()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.ICMPolicy.process_fn"><code class="docutils literal notranslate"><span class="pre">ICMPolicy.process_fn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.ICMPolicy.post_process_fn"><code class="docutils literal notranslate"><span class="pre">ICMPolicy.post_process_fn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.ICMPolicy.learn"><code class="docutils literal notranslate"><span class="pre">ICMPolicy.learn()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#multi-agent">Multi-agent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tianshou.policy.MultiAgentPolicyManager"><code class="docutils literal notranslate"><span class="pre">MultiAgentPolicyManager</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.MultiAgentPolicyManager.replace_policy"><code class="docutils literal notranslate"><span class="pre">MultiAgentPolicyManager.replace_policy()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.MultiAgentPolicyManager.process_fn"><code class="docutils literal notranslate"><span class="pre">MultiAgentPolicyManager.process_fn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.MultiAgentPolicyManager.exploration_noise"><code class="docutils literal notranslate"><span class="pre">MultiAgentPolicyManager.exploration_noise()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.MultiAgentPolicyManager.forward"><code class="docutils literal notranslate"><span class="pre">MultiAgentPolicyManager.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tianshou.policy.MultiAgentPolicyManager.learn"><code class="docutils literal notranslate"><span class="pre">MultiAgentPolicyManager.learn()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tianshou.trainer.html">tianshou.trainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="tianshou.exploration.html">tianshou.exploration</a></li>
<li class="toctree-l1"><a class="reference internal" href="tianshou.utils.html">tianshou.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to Tianshou</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributor.html">Contributor</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Tianshou</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">tianshou.policy</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api/tianshou.policy.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tianshou-policy">
<h1>tianshou.policy<a class="headerlink" href="#tianshou-policy" title="Link to this heading">¶</a></h1>
<section id="base">
<h2>Base<a class="headerlink" href="#base" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">BasePolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">observation_space</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Space</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_space</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Space</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_scaling</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_bound_method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'clip'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'tanh'</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LambdaLR</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="tianshou.utils.html#tianshou.utils.MultipleLRSchedulers" title="tianshou.utils.lr_scheduler.MultipleLRSchedulers"><span class="pre">MultipleLRSchedulers</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">ABC</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>The base class for any RL policy.</p>
<p>Tianshou aims to modularize RL algorithms. It comes into several classes of
policies in Tianshou. All of the policy classes must inherit
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>.</p>
<p>A policy class typically has the following parts:</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code>: initialize the policy, including         coping the target network and so on;</p></li>
<li><p><a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>: compute action with given         observation;</p></li>
<li><p><a class="reference internal" href="#tianshou.policy.BasePolicy.process_fn" title="tianshou.policy.BasePolicy.process_fn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_fn()</span></code></a>: pre-process data from the         replay buffer (this function can interact with replay buffer);</p></li>
<li><p><a class="reference internal" href="#tianshou.policy.BasePolicy.learn" title="tianshou.policy.BasePolicy.learn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">learn()</span></code></a>: update policy with a given batch of         data.</p></li>
<li><p><a class="reference internal" href="#tianshou.policy.BasePolicy.post_process_fn" title="tianshou.policy.BasePolicy.post_process_fn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">post_process_fn()</span></code></a>: update the replay buffer         from the learning process (e.g., prioritized replay buffer needs to update         the weight);</p></li>
<li><p><a class="reference internal" href="#tianshou.policy.BasePolicy.update" title="tianshou.policy.BasePolicy.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a>: the main interface for training,         i.e., <cite>process_fn -&gt; learn -&gt; post_process_fn</cite>.</p></li>
</ul>
<p>Most of the policy needs a neural network to predict the action and an
optimizer to optimize the policy. The rules of self-defined networks are:</p>
<ol class="arabic simple">
<li><p>Input: observation “obs” (may be a <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>, a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, a     dict or any others), hidden state “state” (for RNN usage), and other information     “info” provided by the environment.</p></li>
<li><p>Output: some “logits”, the next hidden state “state”, and the intermediate     result during policy forwarding procedure “policy”. The “logits” could be a tuple     instead of a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>. It depends on how the policy process the network     output. For example, in PPO, the return of the network might be     <code class="docutils literal notranslate"><span class="pre">(mu,</span> <span class="pre">sigma),</span> <span class="pre">state</span></code> for Gaussian policy. The “policy” can be a Batch of     torch.Tensor or other things, which will be stored in the replay buffer, and can     be accessed in the policy update process (e.g. in “policy.learn()”, the     “batch.policy” is what you need).</p></li>
</ol>
<p>Since <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> inherits <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>, you can
use <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> almost the same as <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>,
for instance, loading and saving the model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;policy.pth&quot;</span><span class="p">)</span>
<span class="n">policy</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;policy.pth&quot;</span><span class="p">))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>observation_space</strong> – appears unused.</p></li>
<li><p><strong>action_space</strong> – required for action_scaling.</p></li>
<li><p><strong>action_scaling</strong> – if True, scale the action from [-1, 1] to the range
of action_space. Note that in this case, the action_space must be provided!</p></li>
<li><p><strong>action_bound_method</strong> – </p></li>
<li><p><strong>lr_scheduler</strong> – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.set_agent_id">
<span class="sig-name descname"><span class="pre">set_agent_id</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">agent_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.set_agent_id"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.set_agent_id" title="Link to this definition">¶</a></dt>
<dd><p>Set self.agent_id = agent_id, for MARL.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.exploration_noise">
<span class="sig-name descname"><span class="pre">exploration_noise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.exploration_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.exploration_noise" title="Link to this definition">¶</a></dt>
<dd><p>Modify the action from policy.forward with exploration noise.</p>
<p>NOTE: currently does not add any noise! Needs to be overridden by subclasses
to actually do something.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>act</strong> – a data batch or numpy.ndarray which is the action taken by
policy.forward.</p></li>
<li><p><strong>batch</strong> – the input batch for policy.forward, kept for advanced usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>action in the same form of input “act” but with added exploration
noise.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.soft_update">
<span class="sig-name descname"><span class="pre">soft_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tgt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.soft_update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.soft_update" title="Link to this definition">¶</a></dt>
<dd><p>Softly update the parameters of target module towards the parameters         of source module.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.forward">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which MUST have the following keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> an numpy.ndarray or a torch.Tensor, the action over                 given batch data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> a dict, an numpy.ndarray or a torch.Tensor, the                 internal state of the policy, <code class="docutils literal notranslate"><span class="pre">None</span></code> as default.</p></li>
</ul>
</p>
</dd>
</dl>
<p>Other keys are user-defined. It depends on the algorithm. For example,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># some code</span>
<span class="k">return</span> <span class="n">Batch</span><span class="p">(</span><span class="n">logits</span><span class="o">=...</span><span class="p">,</span> <span class="n">act</span><span class="o">=...</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dist</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<p>The keyword <code class="docutils literal notranslate"><span class="pre">policy</span></code> is reserved and the corresponding data will be
stored into the replay buffer. For instance,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># some code</span>
<span class="k">return</span> <span class="n">Batch</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">Batch</span><span class="p">(</span><span class="n">log_prob</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">act</span><span class="p">)))</span>
<span class="c1"># and in the sampled data batch, you can directly use</span>
<span class="c1"># batch.policy.log_prob to get your data.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In continuous action space, you should do another step “map_action” to get
the real action:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">act</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">act</span>  <span class="c1"># doesn&#39;t map to the target action range</span>
<span class="n">act</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">map_action</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.map_action">
<span class="sig-name descname"><span class="pre">map_action</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">TBatch</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">TBatch</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.map_action"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.map_action" title="Link to this definition">¶</a></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">map_action</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span></dt>
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">map_action</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span></dt>
<dd><p>Map raw network output to action range in gym’s env.action_space.</p>
<p>This function is called in <a class="reference internal" href="tianshou.data.html#tianshou.data.Collector.collect" title="tianshou.data.Collector.collect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">collect()</span></code></a> and only
affects action sending to env. Remapped action will not be stored in buffer
and thus can be viewed as a part of env (a black box action transformation).</p>
<p>Action mapping includes 2 standard procedures: bounding and scaling. Bounding
procedure expects original action range is (-inf, inf) and maps it to [-1, 1],
while scaling procedure expects original action range is (-1, 1) and maps it
to [action_space.low, action_space.high]. Bounding procedure is applied first.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>act</strong> – a data batch or numpy.ndarray which is the action taken by
policy.forward.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>action in the same form of input “act” but remap to the target action
space.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.map_action_inverse">
<span class="sig-name descname"><span class="pre">map_action_inverse</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.map_action_inverse"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.map_action_inverse" title="Link to this definition">¶</a></dt>
<dd><p>Inverse operation to <a class="reference internal" href="#tianshou.policy.BasePolicy.map_action" title="tianshou.policy.BasePolicy.map_action"><code class="xref py py-meth docutils literal notranslate"><span class="pre">map_action()</span></code></a>.</p>
<p>This function is called in <a class="reference internal" href="tianshou.data.html#tianshou.data.Collector.collect" title="tianshou.data.Collector.collect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">collect()</span></code></a> for
random initial steps. It scales [action_space.low, action_space.high] to
the value ranges of policy.forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>act</strong> – a data batch, list or numpy.ndarray which is the action taken
by gym.spaces.Box.sample().</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>action remapped.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RolloutBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Pre-process the data from the provided replay buffer.</p>
<p>Used in <a class="reference internal" href="#tianshou.policy.BasePolicy.update" title="tianshou.policy.BasePolicy.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a>. Check out <a class="reference internal" href="../tutorials/concepts.html#process-fn"><span class="std std-ref">policy.process_fn</span></a> for more information.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.learn">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.post_process_fn">
<span class="sig-name descname"><span class="pre">post_process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">BatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.post_process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.post_process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Post-process the data from the provided replay buffer.</p>
<p>This will only have an effect if the buffer has the
method <cite>update_weight</cite> and the batch has the attribute <cite>weight</cite>.</p>
<p>Typical usage is to update the sampling weight in prioritized
experience replay. Used in <a class="reference internal" href="#tianshou.policy.BasePolicy.update" title="tianshou.policy.BasePolicy.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.update" title="Link to this definition">¶</a></dt>
<dd><p>Update the policy network and replay buffer.</p>
<p>It includes 3 function steps: process_fn, learn, and post_process_fn. In
addition, this function will change the value of <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>: it will be
False before this function and will be True when executing <a class="reference internal" href="#tianshou.policy.BasePolicy.update" title="tianshou.policy.BasePolicy.update"><code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code></a>.
Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more detailed explanation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sample_size</strong> (<em>int</em>) – 0 means it will extract all the data from the buffer,
otherwise it will sample a batch with given sample_size.</p></li>
<li><p><strong>buffer</strong> (<a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.ReplayBuffer"><em>ReplayBuffer</em></a>) – the corresponding replay buffer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dict, including the data needed to be logged (e.g., loss) from
<code class="docutils literal notranslate"><span class="pre">policy.learn()</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.value_mask">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">value_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.value_mask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.value_mask" title="Link to this definition">¶</a></dt>
<dd><p>Value mask determines whether the obs_next of buffer[indices] is valid.</p>
<p>For instance, usually “obs_next” after “done” flag is considered to be invalid,
and its q/advantage value can provide meaningless (even misleading)
information, and should be set to 0 by hand. But if “done” flag is generated
because timelimit of game length (info[“TimeLimit.truncated”] is set to True in
gym’s settings), “obs_next” will instead be valid. Value mask is typically used
for assisting in calculating the correct q/advantage value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>buffer</strong> (<a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.ReplayBuffer"><em>ReplayBuffer</em></a>) – the corresponding replay buffer.</p></li>
<li><p><strong>indices</strong> (<em>numpy.ndarray</em>) – indices of replay buffer whose “obs_next” will be
judged.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A bool type numpy.ndarray in the same shape with indices. “True” means
“obs_next” of that buffer[indices] is valid.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.compute_episodic_return">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">compute_episodic_return</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_s_</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_s</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gae_lambda</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.95</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">ndarray</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.compute_episodic_return"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.compute_episodic_return" title="Link to this definition">¶</a></dt>
<dd><p>Compute returns over given batch.</p>
<p>Use Implementation of Generalized Advantage Estimator (arXiv:1506.02438)
to calculate q/advantage value of given batch. Returns are calculated as
advantage + value, which is exactly equivalent to using <span class="math notranslate nohighlight">\(TD(\lambda)\)</span>
for estimating returns.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – a data batch which contains several episodes of data in
sequential order. Mind that the end of each finished episode of batch
should be marked by done flag, unfinished (or collecting) episodes will be
recognized by buffer.unfinished_index().</p></li>
<li><p><strong>buffer</strong> – the corresponding replay buffer.</p></li>
<li><p><strong>indices</strong> (<em>numpy.ndarray</em>) – tell batch’s location in buffer, batch is equal
to buffer[indices].</p></li>
<li><p><strong>v_s</strong> (<em>np.ndarray</em>) – the value function of all next states <span class="math notranslate nohighlight">\(V(s')\)</span>.
If None, it will be set to an array of 0.</p></li>
<li><p><strong>v_s</strong> – the value function of all current states <span class="math notranslate nohighlight">\(V(s)\)</span>.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – the discount factor, should be in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>gae_lambda</strong> (<em>float</em>) – the parameter for Generalized Advantage Estimation,
should be in [0, 1]. Default to 0.95.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>two numpy arrays (returns, advantage) with each shape (bsz, ).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BasePolicy.compute_nstep_return">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">compute_nstep_return</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_q_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rew_norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchWithReturnsProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/base.html#BasePolicy.compute_nstep_return"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BasePolicy.compute_nstep_return" title="Link to this definition">¶</a></dt>
<dd><p>Compute n-step return for Q-learning targets.</p>
<div class="math notranslate nohighlight">
\[G_t = \sum_{i = t}^{t + n - 1} \gamma^{i - t}(1 - d_i)r_i +
\gamma^n (1 - d_{t + n}) Q_{\mathrm{target}}(s_{t + n})\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount factor, <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span>,
<span class="math notranslate nohighlight">\(d_t\)</span> is the done flag of step <span class="math notranslate nohighlight">\(t\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><em>Batch</em></a>) – a data batch, which is equal to buffer[indices].</p></li>
<li><p><strong>buffer</strong> (<a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.ReplayBuffer"><em>ReplayBuffer</em></a>) – the data buffer.</p></li>
<li><p><strong>indices</strong> – tell batch’s location in buffer</p></li>
<li><p><strong>target_q_fn</strong> (<em>function</em>) – a function which compute target Q value
of “obs_next” given data buffer and wanted indices.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – the discount factor, should be in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>n_step</strong> (<em>int</em>) – the number of estimation step, should be an int greater
than 0. Default to 1.</p></li>
<li><p><strong>rew_norm</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1), Default to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a Batch. The result will be stored in batch.returns as a
torch.Tensor with the same shape as target_q_fn’s return tensor.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.RandomPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">RandomPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">observation_space</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Space</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_space</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Space</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_scaling</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_bound_method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'clip'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'tanh'</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LambdaLR</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="tianshou.utils.html#tianshou.utils.MultipleLRSchedulers" title="tianshou.utils.lr_scheduler.MultipleLRSchedulers"><span class="pre">MultipleLRSchedulers</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/random.html#RandomPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.RandomPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a></p>
<p>A random agent used in multi-agent learning.</p>
<p>It randomly chooses an action from the legal action.</p>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.RandomPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ActBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/random.html#RandomPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.RandomPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute the random action over the given batch data.</p>
<p>The input should contain a mask in batch.obs, with “True” to be
available and “False” to be unavailable. For example,
<code class="docutils literal notranslate"><span class="pre">batch.obs.mask</span> <span class="pre">==</span> <span class="pre">np.array([[False,</span> <span class="pre">True,</span> <span class="pre">False]])</span></code> means with batch
size 1, action “1” is available but action “0” and “2” are unavailable.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> with “act” key, containing
the random action.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.RandomPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/random.html#RandomPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.RandomPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Since a random agent learns nothing, it returns an empty dict.</p>
</dd></dl>

</dd></dl>

</section>
<section id="model-free">
<h2>Model-free<a class="headerlink" href="#model-free" title="Link to this heading">¶</a></h2>
<section id="dqn-family">
<h3>DQN Family<a class="headerlink" href="#dqn-family" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.DQNPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">DQNPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_freq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_double</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_loss_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/dqn.html#DQNPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DQNPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a></p>
<p>Implementation of Deep Q Network. arXiv:1312.5602.</p>
<p>Implementation of Double Q-Learning. arXiv:1509.06461.</p>
<p>Implementation of Dueling DQN. arXiv:1511.06581 (the dueling DQN is
implemented in the network side, not here).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1].</p></li>
<li><p><strong>estimation_step</strong> (<em>int</em>) – the number of steps to look ahead. Default to 1.</p></li>
<li><p><strong>target_update_freq</strong> (<em>int</em>) – the target network update frequency (0 if
you do not use the target network). Default to 0.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>is_double</strong> (<em>bool</em>) – use double dqn. Default to True.</p></li>
<li><p><strong>clip_loss_grad</strong> (<em>bool</em>) – clip the gradient of the loss in accordance
with nature14236; this amounts to using the Huber loss instead of
the MSE loss. Default to False.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DQNPolicy.set_eps">
<span class="sig-name descname"><span class="pre">set_eps</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/dqn.html#DQNPolicy.set_eps"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DQNPolicy.set_eps" title="Link to this definition">¶</a></dt>
<dd><p>Set the eps for epsilon-greedy exploration.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DQNPolicy.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#tianshou.policy.DQNPolicy" title="tianshou.policy.modelfree.dqn.DQNPolicy"><span class="pre">DQNPolicy</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/dqn.html#DQNPolicy.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DQNPolicy.train" title="Link to this definition">¶</a></dt>
<dd><p>Set the module in training mode, except for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DQNPolicy.sync_weight">
<span class="sig-name descname"><span class="pre">sync_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/dqn.html#DQNPolicy.sync_weight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DQNPolicy.sync_weight" title="Link to this definition">¶</a></dt>
<dd><p>Synchronize the weight for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DQNPolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchWithReturnsProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/dqn.html#DQNPolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DQNPolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Compute the n-step return for Q-learning targets.</p>
<p>More details can be found at
<a class="reference internal" href="#tianshou.policy.BasePolicy.compute_nstep_return" title="tianshou.policy.BasePolicy.compute_nstep_return"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_nstep_return()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DQNPolicy.compute_q_value">
<span class="sig-name descname"><span class="pre">compute_q_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/dqn.html#DQNPolicy.compute_q_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DQNPolicy.compute_q_value" title="Link to this definition">¶</a></dt>
<dd><p>Compute the q value based on the network’s raw output and action mask.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DQNPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'model'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'obs'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ModelOutputBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/dqn.html#DQNPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DQNPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<p>If you need to mask the action, please add a “mask” into batch.obs, for
example, if we have an environment that has “0/1/2” three actions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="o">==</span> <span class="n">Batch</span><span class="p">(</span>
    <span class="n">obs</span><span class="o">=</span><span class="n">Batch</span><span class="p">(</span>
        <span class="n">obs</span><span class="o">=</span><span class="s2">&quot;original obs, with batch_size=1 for demonstration&quot;</span><span class="p">,</span>
        <span class="n">mask</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]]),</span>
        <span class="c1"># action 1 is available</span>
        <span class="c1"># action 0 and 2 are unavailable</span>
    <span class="p">),</span>
    <span class="o">...</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which has 3 keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logits</span></code> the network’s raw output.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> the hidden state.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DQNPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/dqn.html#DQNPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DQNPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DQNPolicy.exploration_noise">
<span class="sig-name descname"><span class="pre">exploration_noise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/dqn.html#DQNPolicy.exploration_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DQNPolicy.exploration_noise" title="Link to this definition">¶</a></dt>
<dd><p>Modify the action from policy.forward with exploration noise.</p>
<p>NOTE: currently does not add any noise! Needs to be overridden by subclasses
to actually do something.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>act</strong> – a data batch or numpy.ndarray which is the action taken by
policy.forward.</p></li>
<li><p><strong>batch</strong> – the input batch for policy.forward, kept for advanced usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>action in the same form of input “act” but with added exploration
noise.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.BranchingDQNPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">BranchingDQNPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.common.BranchingNet" title="tianshou.utils.net.common.BranchingNet"><span class="pre">BranchingNet</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_freq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_double</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/bdq.html#BranchingDQNPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BranchingDQNPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.DQNPolicy" title="tianshou.policy.modelfree.dqn.DQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DQNPolicy</span></code></a></p>
<p>Implementation of the Branching dual Q network arXiv:1711.08946.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1].</p></li>
<li><p><strong>estimation_step</strong> (<em>int</em>) – the number of steps to look ahead. Default to 1.</p></li>
<li><p><strong>target_update_freq</strong> (<em>int</em>) – the target network update frequency (0 if
you do not use the target network). Default to 0.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>is_double</strong> (<em>bool</em>) – use double network. Default to True.</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BranchingDQNPolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchWithReturnsProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/bdq.html#BranchingDQNPolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BranchingDQNPolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Compute the 1-step return for BDQ targets.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BranchingDQNPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'model'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'obs'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ModelOutputBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/bdq.html#BranchingDQNPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BranchingDQNPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<p>If you need to mask the action, please add a “mask” into batch.obs, for
example, if we have an environment that has “0/1/2” three actions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="o">==</span> <span class="n">Batch</span><span class="p">(</span>
    <span class="n">obs</span><span class="o">=</span><span class="n">Batch</span><span class="p">(</span>
        <span class="n">obs</span><span class="o">=</span><span class="s2">&quot;original obs, with batch_size=1 for demonstration&quot;</span><span class="p">,</span>
        <span class="n">mask</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]]),</span>
        <span class="c1"># action 1 is available</span>
        <span class="c1"># action 0 and 2 are unavailable</span>
    <span class="p">),</span>
    <span class="o">...</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which has 3 keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logits</span></code> the network’s raw output.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> the hidden state.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BranchingDQNPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/bdq.html#BranchingDQNPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BranchingDQNPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BranchingDQNPolicy.exploration_noise">
<span class="sig-name descname"><span class="pre">exploration_noise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/bdq.html#BranchingDQNPolicy.exploration_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BranchingDQNPolicy.exploration_noise" title="Link to this definition">¶</a></dt>
<dd><p>Modify the action from policy.forward with exploration noise.</p>
<p>NOTE: currently does not add any noise! Needs to be overridden by subclasses
to actually do something.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>act</strong> – a data batch or numpy.ndarray which is the action taken by
policy.forward.</p></li>
<li><p><strong>batch</strong> – the input batch for policy.forward, kept for advanced usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>action in the same form of input “act” but with added exploration
noise.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.C51Policy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">C51Policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">51</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-10.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_freq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/c51.html#C51Policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.C51Policy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.DQNPolicy" title="tianshou.policy.modelfree.dqn.DQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DQNPolicy</span></code></a></p>
<p>Implementation of Categorical Deep Q-Network. arXiv:1707.06887.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1].</p></li>
<li><p><strong>num_atoms</strong> (<em>int</em>) – the number of atoms in the support set of the
value distribution. Default to 51.</p></li>
<li><p><strong>v_min</strong> (<em>float</em>) – the value of the smallest atom in the support set.
Default to -10.0.</p></li>
<li><p><strong>v_max</strong> (<em>float</em>) – the value of the largest atom in the support set.
Default to 10.0.</p></li>
<li><p><strong>estimation_step</strong> (<em>int</em>) – the number of steps to look ahead. Default to 1.</p></li>
<li><p><strong>target_update_freq</strong> (<em>int</em>) – the target network update frequency (0 if
you do not use the target network). Default to 0.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.DQNPolicy" title="tianshou.policy.DQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DQNPolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.C51Policy.compute_q_value">
<span class="sig-name descname"><span class="pre">compute_q_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/c51.html#C51Policy.compute_q_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.C51Policy.compute_q_value" title="Link to this definition">¶</a></dt>
<dd><p>Compute the q value based on the network’s raw output and action mask.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.C51Policy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/c51.html#C51Policy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.C51Policy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.RainbowPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">RainbowPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">51</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-10.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_freq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/rainbow.html#RainbowPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.RainbowPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.C51Policy" title="tianshou.policy.modelfree.c51.C51Policy"><code class="xref py py-class docutils literal notranslate"><span class="pre">C51Policy</span></code></a></p>
<p>Implementation of Rainbow DQN. arXiv:1710.02298.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1].</p></li>
<li><p><strong>num_atoms</strong> (<em>int</em>) – the number of atoms in the support set of the
value distribution. Default to 51.</p></li>
<li><p><strong>v_min</strong> (<em>float</em>) – the value of the smallest atom in the support set.
Default to -10.0.</p></li>
<li><p><strong>v_max</strong> (<em>float</em>) – the value of the largest atom in the support set.
Default to 10.0.</p></li>
<li><p><strong>estimation_step</strong> (<em>int</em>) – the number of steps to look ahead. Default to 1.</p></li>
<li><p><strong>target_update_freq</strong> (<em>int</em>) – the target network update frequency (0 if
you do not use the target network). Default to 0.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.C51Policy" title="tianshou.policy.C51Policy"><code class="xref py py-class docutils literal notranslate"><span class="pre">C51Policy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.RainbowPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/rainbow.html#RainbowPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.RainbowPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.QRDQNPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">QRDQNPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_quantiles</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_freq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/qrdqn.html#QRDQNPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.QRDQNPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.DQNPolicy" title="tianshou.policy.modelfree.dqn.DQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DQNPolicy</span></code></a></p>
<p>Implementation of Quantile Regression Deep Q-Network. arXiv:1710.10044.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1].</p></li>
<li><p><strong>num_quantiles</strong> (<em>int</em>) – the number of quantile midpoints in the inverse
cumulative distribution function of the value. Default to 200.</p></li>
<li><p><strong>estimation_step</strong> (<em>int</em>) – the number of steps to look ahead. Default to 1.</p></li>
<li><p><strong>target_update_freq</strong> (<em>int</em>) – the target network update frequency (0 if
you do not use the target network).</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.DQNPolicy" title="tianshou.policy.DQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DQNPolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.QRDQNPolicy.compute_q_value">
<span class="sig-name descname"><span class="pre">compute_q_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logits</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/qrdqn.html#QRDQNPolicy.compute_q_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.QRDQNPolicy.compute_q_value" title="Link to this definition">¶</a></dt>
<dd><p>Compute the q value based on the network’s raw output and action mask.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.QRDQNPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/qrdqn.html#QRDQNPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.QRDQNPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.IQNPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">IQNPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">online_sample_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_sample_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_freq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/iqn.html#IQNPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.IQNPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.QRDQNPolicy" title="tianshou.policy.modelfree.qrdqn.QRDQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">QRDQNPolicy</span></code></a></p>
<p>Implementation of Implicit Quantile Network. arXiv:1806.06923.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1].</p></li>
<li><p><strong>sample_size</strong> (<em>int</em>) – the number of samples for policy evaluation.
Default to 32.</p></li>
<li><p><strong>online_sample_size</strong> (<em>int</em>) – the number of samples for online model
in training. Default to 8.</p></li>
<li><p><strong>target_sample_size</strong> (<em>int</em>) – the number of samples for target model
in training. Default to 8.</p></li>
<li><p><strong>estimation_step</strong> (<em>int</em>) – the number of steps to look ahead. Default to 1.</p></li>
<li><p><strong>target_update_freq</strong> (<em>int</em>) – the target network update frequency (0 if
you do not use the target network).</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.QRDQNPolicy" title="tianshou.policy.QRDQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">QRDQNPolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.IQNPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'model'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'obs'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">QuantileRegressionBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/iqn.html#IQNPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.IQNPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<p>If you need to mask the action, please add a “mask” into batch.obs, for
example, if we have an environment that has “0/1/2” three actions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="o">==</span> <span class="n">Batch</span><span class="p">(</span>
    <span class="n">obs</span><span class="o">=</span><span class="n">Batch</span><span class="p">(</span>
        <span class="n">obs</span><span class="o">=</span><span class="s2">&quot;original obs, with batch_size=1 for demonstration&quot;</span><span class="p">,</span>
        <span class="n">mask</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]]),</span>
        <span class="c1"># action 1 is available</span>
        <span class="c1"># action 0 and 2 are unavailable</span>
    <span class="p">),</span>
    <span class="o">...</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which has 3 keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logits</span></code> the network’s raw output.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> the hidden state.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.IQNPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/iqn.html#IQNPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.IQNPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.FQFPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">FQFPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.discrete.FullQuantileFunction" title="tianshou.utils.net.discrete.FullQuantileFunction"><span class="pre">FullQuantileFunction</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fraction_model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.discrete.FractionProposalNetwork" title="tianshou.utils.net.discrete.FractionProposalNetwork"><span class="pre">FractionProposalNetwork</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">fraction_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_fractions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ent_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_freq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/fqf.html#FQFPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.FQFPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.QRDQNPolicy" title="tianshou.policy.modelfree.qrdqn.QRDQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">QRDQNPolicy</span></code></a></p>
<p>Implementation of Fully-parameterized Quantile Function. arXiv:1911.02140.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>fraction_model</strong> (<a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.discrete.FractionProposalNetwork" title="tianshou.utils.net.discrete.FractionProposalNetwork"><em>FractionProposalNetwork</em></a>) – a FractionProposalNetwork for
proposing fractions/quantiles given state.</p></li>
<li><p><strong>fraction_optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing
the fraction model above.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1].</p></li>
<li><p><strong>num_fractions</strong> (<em>int</em>) – the number of fractions to use. Default to 32.</p></li>
<li><p><strong>ent_coef</strong> (<em>float</em>) – the coefficient for entropy loss. Default to 0.</p></li>
<li><p><strong>estimation_step</strong> (<em>int</em>) – the number of steps to look ahead. Default to 1.</p></li>
<li><p><strong>target_update_freq</strong> (<em>int</em>) – the target network update frequency (0 if
you do not use the target network).</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.QRDQNPolicy" title="tianshou.policy.QRDQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">QRDQNPolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.FQFPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'model'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'obs'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fractions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">FQFBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/fqf.html#FQFPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.FQFPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<p>If you need to mask the action, please add a “mask” into batch.obs, for
example, if we have an environment that has “0/1/2” three actions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="o">==</span> <span class="n">Batch</span><span class="p">(</span>
    <span class="n">obs</span><span class="o">=</span><span class="n">Batch</span><span class="p">(</span>
        <span class="n">obs</span><span class="o">=</span><span class="s2">&quot;original obs, with batch_size=1 for demonstration&quot;</span><span class="p">,</span>
        <span class="n">mask</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]]),</span>
        <span class="c1"># action 1 is available</span>
        <span class="c1"># action 0 and 2 are unavailable</span>
    <span class="p">),</span>
    <span class="o">...</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which has 3 keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logits</span></code> the network’s raw output.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> the hidden state.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.FQFPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/fqf.html#FQFPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.FQFPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="on-policy">
<h3>On-policy<a class="headerlink" href="#on-policy" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.PGPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">PGPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Distribution</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_scaling</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_bound_method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'clip'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'tanh'</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'clip'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deterministic_eval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/pg.html#PGPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.PGPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a></p>
<p>Implementation of REINFORCE algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>dist_fn</strong> – distribution class for computing the action.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action), “tanh” (for applying tanh
squashing) for now, or empty string for no bounding. Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
<li><p><strong>deterministic_eval</strong> (<em>bool</em>) – whether to use deterministic action instead of
stochastic action sampled by the policy. Default to False.</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.PGPolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchWithReturnsProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/pg.html#PGPolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.PGPolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Compute the discounted returns (Monte Carlo estimates) for each transition.</p>
<p>They are added to the batch under the field <cite>returns</cite>.
Note: this function will modify the input batch!</p>
<div class="math notranslate nohighlight">
\[G_t = \sum_{i=t}^T \gamma^{i-t}r_i\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the terminal time step, <span class="math notranslate nohighlight">\(\gamma\)</span> is the
discount factor, <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – a data batch which contains several episodes of data in
sequential order. Mind that the end of each finished episode of batch
should be marked by done flag, unfinished (or collecting) episodes will be
recognized by buffer.unfinished_index().</p></li>
<li><p><strong>buffer</strong> – the corresponding replay buffer.</p></li>
<li><p><strong>indices</strong> (<em>numpy.ndarray</em>) – tell batch’s location in buffer, batch is equal
to buffer[indices].</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.PGPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DistBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/pg.html#PGPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.PGPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data by applying the actor.</p>
<p>Will sample from the dist_fn, if appropriate.
Returns a new object representing the processed batch data
(contrary to other methods that modify the input batch inplace).</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.PGPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repeat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/pg.html#PGPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.PGPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.NPGPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">NPGPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Distribution</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">advantage_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim_critic_iters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_step_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/npg.html#NPGPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.NPGPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.A2CPolicy" title="tianshou.policy.modelfree.a2c.A2CPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">A2CPolicy</span></code></a></p>
<p>Implementation of Natural Policy Gradient.</p>
<p><a class="reference external" href="https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf">https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>critic</strong> (<em>torch.nn.Module</em>) – the critic network. (s -&gt; V(s))</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor and critic network.</p></li>
<li><p><strong>dist_fn</strong> – distribution class for computing the action.</p></li>
<li><p><strong>advantage_normalization</strong> (<em>bool</em>) – whether to do per mini-batch advantage
normalization. Default to True.</p></li>
<li><p><strong>optim_critic_iters</strong> (<em>int</em>) – Number of times to optimize critic network per
update. Default to 5.</p></li>
<li><p><strong>gae_lambda</strong> (<em>float</em>) – in [0, 1], param for Generalized Advantage Estimation.
Default to 0.95.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize estimated values to have std close to
1. Default to False.</p></li>
<li><p><strong>max_batchsize</strong> (<em>int</em>) – the maximum size of the batch when computing GAE,
depends on the size of available memory and the memory cost of the
model; should be as large as possible within the memory constraint.
Default to 256.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action), “tanh” (for applying tanh
squashing) for now, or empty string for no bounding. Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
<li><p><strong>deterministic_eval</strong> (<em>bool</em>) – whether to use deterministic action instead of
stochastic action sampled by the policy. Default to False.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.NPGPolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchWithAdvantagesProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/npg.html#NPGPolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.NPGPolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Compute the discounted returns (Monte Carlo estimates) for each transition.</p>
<p>They are added to the batch under the field <cite>returns</cite>.
Note: this function will modify the input batch!</p>
<div class="math notranslate nohighlight">
\[G_t = \sum_{i=t}^T \gamma^{i-t}r_i\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the terminal time step, <span class="math notranslate nohighlight">\(\gamma\)</span> is the
discount factor, <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – a data batch which contains several episodes of data in
sequential order. Mind that the end of each finished episode of batch
should be marked by done flag, unfinished (or collecting) episodes will be
recognized by buffer.unfinished_index().</p></li>
<li><p><strong>buffer</strong> – the corresponding replay buffer.</p></li>
<li><p><strong>indices</strong> (<em>numpy.ndarray</em>) – tell batch’s location in buffer, batch is equal
to buffer[indices].</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.NPGPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repeat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/npg.html#NPGPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.NPGPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.A2CPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">A2CPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Distribution</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vf_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ent_coef</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_grad_norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gae_lambda</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.95</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_batchsize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/a2c.html#A2CPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.A2CPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.PGPolicy" title="tianshou.policy.modelfree.pg.PGPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PGPolicy</span></code></a></p>
<p>Implementation of Synchronous Advantage Actor-Critic. arXiv:1602.01783.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>critic</strong> (<em>torch.nn.Module</em>) – the critic network. (s -&gt; V(s))</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor and critic network.</p></li>
<li><p><strong>dist_fn</strong> – distribution class for computing the action.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>vf_coef</strong> (<em>float</em>) – weight for value loss. Default to 0.5.</p></li>
<li><p><strong>ent_coef</strong> (<em>float</em>) – weight for entropy loss. Default to 0.01.</p></li>
<li><p><strong>max_grad_norm</strong> (<em>float</em>) – clipping gradients in back propagation. Default to
None.</p></li>
<li><p><strong>gae_lambda</strong> (<em>float</em>) – in [0, 1], param for Generalized Advantage Estimation.
Default to 0.95.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize estimated values to have std close to
1. Default to False.</p></li>
<li><p><strong>max_batchsize</strong> (<em>int</em>) – the maximum size of the batch when computing GAE,
depends on the size of available memory and the memory cost of the
model; should be as large as possible within the memory constraint.
Default to 256.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action), “tanh” (for applying tanh
squashing) for now, or empty string for no bounding. Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
<li><p><strong>deterministic_eval</strong> (<em>bool</em>) – whether to use deterministic action instead of
stochastic action sampled by the policy. Default to False.</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.A2CPolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchWithAdvantagesProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/a2c.html#A2CPolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.A2CPolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Compute the discounted returns (Monte Carlo estimates) for each transition.</p>
<p>They are added to the batch under the field <cite>returns</cite>.
Note: this function will modify the input batch!</p>
<div class="math notranslate nohighlight">
\[G_t = \sum_{i=t}^T \gamma^{i-t}r_i\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the terminal time step, <span class="math notranslate nohighlight">\(\gamma\)</span> is the
discount factor, <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – a data batch which contains several episodes of data in
sequential order. Mind that the end of each finished episode of batch
should be marked by done flag, unfinished (or collecting) episodes will be
recognized by buffer.unfinished_index().</p></li>
<li><p><strong>buffer</strong> – the corresponding replay buffer.</p></li>
<li><p><strong>indices</strong> (<em>numpy.ndarray</em>) – tell batch’s location in buffer, batch is equal
to buffer[indices].</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.A2CPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repeat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/a2c.html#A2CPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.A2CPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.TRPOPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">TRPOPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Distribution</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_kl</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backtrack_coeff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_backtracks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/trpo.html#TRPOPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.TRPOPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.NPGPolicy" title="tianshou.policy.modelfree.npg.NPGPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">NPGPolicy</span></code></a></p>
<p>Implementation of Trust Region Policy Optimization. arXiv:1502.05477.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>critic</strong> (<em>torch.nn.Module</em>) – the critic network. (s -&gt; V(s))</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor and critic network.</p></li>
<li><p><strong>dist_fn</strong> – distribution class for computing the action.</p></li>
<li><p><strong>advantage_normalization</strong> (<em>bool</em>) – whether to do per mini-batch advantage
normalization. Default to True.</p></li>
<li><p><strong>optim_critic_iters</strong> (<em>int</em>) – Number of times to optimize critic network per
update. Default to 5.</p></li>
<li><p><strong>max_kl</strong> (<em>int</em>) – max kl-divergence used to constrain each actor network update.
Default to 0.01.</p></li>
<li><p><strong>backtrack_coeff</strong> (<em>float</em>) – Coefficient to be multiplied by step size when
constraints are not met. Default to 0.8.</p></li>
<li><p><strong>max_backtracks</strong> (<em>int</em>) – Max number of backtracking times in linesearch. Default
to 10.</p></li>
<li><p><strong>gae_lambda</strong> (<em>float</em>) – in [0, 1], param for Generalized Advantage Estimation.
Default to 0.95.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize estimated values to have std close to
1. Default to False.</p></li>
<li><p><strong>max_batchsize</strong> (<em>int</em>) – the maximum size of the batch when computing GAE,
depends on the size of available memory and the memory cost of the
model; should be as large as possible within the memory constraint.
Default to 256.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action), “tanh” (for applying tanh
squashing) for now, or empty string for no bounding. Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
<li><p><strong>deterministic_eval</strong> (<em>bool</em>) – whether to use deterministic action instead of
stochastic action sampled by the policy. Default to False.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.TRPOPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repeat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/trpo.html#TRPOPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.TRPOPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.PPOPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">PPOPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Distribution</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps_clip</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual_clip</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_clip</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">advantage_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recompute_advantage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ppo.html#PPOPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.PPOPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.A2CPolicy" title="tianshou.policy.modelfree.a2c.A2CPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">A2CPolicy</span></code></a></p>
<p>Implementation of Proximal Policy Optimization. arXiv:1707.06347.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>critic</strong> (<em>torch.nn.Module</em>) – the critic network. (s -&gt; V(s))</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor and critic network.</p></li>
<li><p><strong>dist_fn</strong> – distribution class for computing the action.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>eps_clip</strong> (<em>float</em>) – <span class="math notranslate nohighlight">\(\epsilon\)</span> in <span class="math notranslate nohighlight">\(L_{CLIP}\)</span> in the original
paper. Default to 0.2.</p></li>
<li><p><strong>dual_clip</strong> (<em>float</em>) – a parameter c mentioned in arXiv:1912.09729 Equ. 5,
where c &gt; 1 is a constant indicating the lower bound.
Default to 5.0 (set None if you do not want to use it).</p></li>
<li><p><strong>value_clip</strong> (<em>bool</em>) – a parameter mentioned in arXiv:1811.02553v3 Sec. 4.1.
Default to True.</p></li>
<li><p><strong>advantage_normalization</strong> (<em>bool</em>) – whether to do per mini-batch advantage
normalization. Default to True.</p></li>
<li><p><strong>recompute_advantage</strong> (<em>bool</em>) – whether to recompute advantage every update
repeat according to <a class="reference external" href="https://arxiv.org/pdf/2006.05990.pdf">https://arxiv.org/pdf/2006.05990.pdf</a> Sec. 3.5.
Default to False.</p></li>
<li><p><strong>vf_coef</strong> (<em>float</em>) – weight for value loss. Default to 0.5.</p></li>
<li><p><strong>ent_coef</strong> (<em>float</em>) – weight for entropy loss. Default to 0.01.</p></li>
<li><p><strong>max_grad_norm</strong> (<em>float</em>) – clipping gradients in back propagation. Default to
None.</p></li>
<li><p><strong>gae_lambda</strong> (<em>float</em>) – in [0, 1], param for Generalized Advantage Estimation.
Default to 0.95.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize estimated values to have std close
to 1, also normalize the advantage to Normal(0, 1). Default to False.</p></li>
<li><p><strong>max_batchsize</strong> (<em>int</em>) – the maximum size of the batch when computing GAE,
depends on the size of available memory and the memory cost of the model;
should be as large as possible within the memory constraint. Default to 256.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action), “tanh” (for applying tanh
squashing) for now, or empty string for no bounding. Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
<li><p><strong>deterministic_eval</strong> (<em>bool</em>) – whether to use deterministic action instead of
stochastic action sampled by the policy. Default to False.</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.PPOPolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LogpOldProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ppo.html#PPOPolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.PPOPolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Compute the discounted returns (Monte Carlo estimates) for each transition.</p>
<p>They are added to the batch under the field <cite>returns</cite>.
Note: this function will modify the input batch!</p>
<div class="math notranslate nohighlight">
\[G_t = \sum_{i=t}^T \gamma^{i-t}r_i\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the terminal time step, <span class="math notranslate nohighlight">\(\gamma\)</span> is the
discount factor, <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – a data batch which contains several episodes of data in
sequential order. Mind that the end of each finished episode of batch
should be marked by done flag, unfinished (or collecting) episodes will be
recognized by buffer.unfinished_index().</p></li>
<li><p><strong>buffer</strong> – the corresponding replay buffer.</p></li>
<li><p><strong>indices</strong> (<em>numpy.ndarray</em>) – tell batch’s location in buffer, batch is equal
to buffer[indices].</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.PPOPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repeat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ppo.html#PPOPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.PPOPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="off-policy">
<h3>Off-policy<a class="headerlink" href="#off-policy" title="Link to this heading">¶</a></h3>
<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.DDPGPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">DDPGPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">|</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_optim:</span> <span class="pre">~torch.optim.optimizer.Optimizer</span> <span class="pre">|</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">|</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic_optim:</span> <span class="pre">~torch.optim.optimizer.Optimizer</span> <span class="pre">|</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exploration_noise:</span> <span class="pre">~tianshou.exploration.random.BaseNoise</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;tianshou.exploration.random.GaussianNoise</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_scaling:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">action_bound_method:</span> <span class="pre">~typing.Literal['clip'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">'tanh']</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">'clip'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs:</span> <span class="pre">~typing.Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ddpg.html#DDPGPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DDPGPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a></p>
<p>Implementation of Deep Deterministic Policy Gradient. arXiv:1509.02971.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>actor_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor network.</p></li>
<li><p><strong>critic</strong> (<em>torch.nn.Module</em>) – the critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for critic network.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – param for soft update of the target network. Default to 0.005.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – discount factor, in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>exploration_noise</strong> (<a class="reference internal" href="tianshou.exploration.html#tianshou.exploration.BaseNoise" title="tianshou.exploration.BaseNoise"><em>BaseNoise</em></a>) – the exploration noise,
add to the action. Default to <code class="docutils literal notranslate"><span class="pre">GaussianNoise(sigma=0.1)</span></code>.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1),
Default to False.</p></li>
<li><p><strong>estimation_step</strong> (<em>int</em>) – the number of steps to look ahead. Default to 1.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action) or empty string for no bounding.
Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DDPGPolicy.set_exp_noise">
<span class="sig-name descname"><span class="pre">set_exp_noise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">noise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.exploration.html#tianshou.exploration.BaseNoise" title="tianshou.exploration.random.BaseNoise"><span class="pre">BaseNoise</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ddpg.html#DDPGPolicy.set_exp_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DDPGPolicy.set_exp_noise" title="Link to this definition">¶</a></dt>
<dd><p>Set the exploration noise.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DDPGPolicy.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#tianshou.policy.DDPGPolicy" title="tianshou.policy.modelfree.ddpg.DDPGPolicy"><span class="pre">DDPGPolicy</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ddpg.html#DDPGPolicy.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DDPGPolicy.train" title="Link to this definition">¶</a></dt>
<dd><p>Set the module in training mode, except for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DDPGPolicy.sync_weight">
<span class="sig-name descname"><span class="pre">sync_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ddpg.html#DDPGPolicy.sync_weight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DDPGPolicy.sync_weight" title="Link to this definition">¶</a></dt>
<dd><p>Soft-update the weight for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DDPGPolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RolloutBatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchWithReturnsProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ddpg.html#DDPGPolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DDPGPolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Pre-process the data from the provided replay buffer.</p>
<p>Used in <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code>. Check out <a class="reference internal" href="../tutorials/concepts.html#process-fn"><span class="std std-ref">policy.process_fn</span></a> for more information.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DDPGPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'actor'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'actor_old'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'actor'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'obs'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ddpg.html#DDPGPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DDPGPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which has 2 keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> the hidden state.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DDPGPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ddpg.html#DDPGPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DDPGPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DDPGPolicy.exploration_noise">
<span class="sig-name descname"><span class="pre">exploration_noise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/ddpg.html#DDPGPolicy.exploration_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DDPGPolicy.exploration_noise" title="Link to this definition">¶</a></dt>
<dd><p>Modify the action from policy.forward with exploration noise.</p>
<p>NOTE: currently does not add any noise! Needs to be overridden by subclasses
to actually do something.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>act</strong> – a data batch or numpy.ndarray which is the action taken by
policy.forward.</p></li>
<li><p><strong>batch</strong> – the input batch for policy.forward, kept for advanced usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>action in the same form of input “act” but with added exploration
noise.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.TD3Policy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">TD3Policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor:</span> <span class="pre">~torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_optim:</span> <span class="pre">~torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1:</span> <span class="pre">~torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1_optim:</span> <span class="pre">~torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2:</span> <span class="pre">~torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2_optim:</span> <span class="pre">~torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exploration_noise:</span> <span class="pre">~tianshou.exploration.random.BaseNoise</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;tianshou.exploration.random.GaussianNoise</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_noise:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_actor_freq:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_clip:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs:</span> <span class="pre">~typing.Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/td3.html#TD3Policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.TD3Policy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.DDPGPolicy" title="tianshou.policy.modelfree.ddpg.DDPGPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DDPGPolicy</span></code></a></p>
<p>Implementation of TD3, arXiv:1802.09477.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>actor_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor network.</p></li>
<li><p><strong>critic1</strong> (<em>torch.nn.Module</em>) – the first critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic1_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the first
critic network.</p></li>
<li><p><strong>critic2</strong> (<em>torch.nn.Module</em>) – the second critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic2_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the second
critic network.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – param for soft update of the target network. Default to 0.005.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – discount factor, in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>exploration_noise</strong> (<em>float</em>) – the exploration noise, add to the action.
Default to <code class="docutils literal notranslate"><span class="pre">GaussianNoise(sigma=0.1)</span></code></p></li>
<li><p><strong>policy_noise</strong> (<em>float</em>) – the noise used in updating policy network.
Default to 0.2.</p></li>
<li><p><strong>update_actor_freq</strong> (<em>int</em>) – the update frequency of actor network.
Default to 2.</p></li>
<li><p><strong>noise_clip</strong> (<em>float</em>) – the clipping range used in updating policy network.
Default to 0.5.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action) or empty string for no bounding.
Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.TD3Policy.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#tianshou.policy.TD3Policy" title="tianshou.policy.modelfree.td3.TD3Policy"><span class="pre">TD3Policy</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/td3.html#TD3Policy.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.TD3Policy.train" title="Link to this definition">¶</a></dt>
<dd><p>Set the module in training mode, except for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.TD3Policy.sync_weight">
<span class="sig-name descname"><span class="pre">sync_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/td3.html#TD3Policy.sync_weight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.TD3Policy.sync_weight" title="Link to this definition">¶</a></dt>
<dd><p>Soft-update the weight for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.TD3Policy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/td3.html#TD3Policy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.TD3Policy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.SACPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">SACPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optimizer</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exploration_noise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.exploration.html#tianshou.exploration.BaseNoise" title="tianshou.exploration.random.BaseNoise"><span class="pre">BaseNoise</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deterministic_eval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/sac.html#SACPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.SACPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.DDPGPolicy" title="tianshou.policy.modelfree.ddpg.DDPGPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DDPGPolicy</span></code></a></p>
<p>Implementation of Soft Actor-Critic. arXiv:1812.05905.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>actor_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor network.</p></li>
<li><p><strong>critic1</strong> (<em>torch.nn.Module</em>) – the first critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic1_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the first
critic network.</p></li>
<li><p><strong>critic2</strong> (<em>torch.nn.Module</em>) – the second critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic2_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the second
critic network.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – param for soft update of the target network. Default to 0.005.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – discount factor, in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>alpha</strong> (<em>(</em><em>float</em><em>, </em><em>torch.Tensor</em><em>, </em><em>torch.optim.Optimizer</em><em>) or </em><em>float</em>) – entropy
regularization coefficient. Default to 0.2.
If a tuple (target_entropy, log_alpha, alpha_optim) is provided, then
alpha is automatically tuned.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>exploration_noise</strong> (<a class="reference internal" href="tianshou.exploration.html#tianshou.exploration.BaseNoise" title="tianshou.exploration.BaseNoise"><em>BaseNoise</em></a>) – add a noise to action for exploration.
Default to None. This is useful when solving hard-exploration problem.</p></li>
<li><p><strong>deterministic_eval</strong> (<em>bool</em>) – whether to use deterministic action (mean
of Gaussian policy) instead of stochastic action sampled by the policy.
Default to True.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action) or empty string for no bounding.
Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.SACPolicy.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#tianshou.policy.SACPolicy" title="tianshou.policy.modelfree.sac.SACPolicy"><span class="pre">SACPolicy</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/sac.html#SACPolicy.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.SACPolicy.train" title="Link to this definition">¶</a></dt>
<dd><p>Set the module in training mode, except for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.SACPolicy.sync_weight">
<span class="sig-name descname"><span class="pre">sync_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/sac.html#SACPolicy.sync_weight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.SACPolicy.sync_weight" title="Link to this definition">¶</a></dt>
<dd><p>Soft-update the weight for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.SACPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'obs'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DistLogProbBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/sac.html#SACPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.SACPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which has 2 keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> the hidden state.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.SACPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/sac.html#SACPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.SACPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.REDQPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">REDQPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critics_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ensemble_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subset_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optimizer</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_delay</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exploration_noise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.exploration.html#tianshou.exploration.BaseNoise" title="tianshou.exploration.random.BaseNoise"><span class="pre">BaseNoise</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deterministic_eval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'min'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/redq.html#REDQPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.REDQPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.DDPGPolicy" title="tianshou.policy.modelfree.ddpg.DDPGPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DDPGPolicy</span></code></a></p>
<p>Implementation of REDQ. arXiv:2101.05982.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>actor_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor network.</p></li>
<li><p><strong>critics</strong> (<em>torch.nn.Module</em>) – critic ensemble networks.</p></li>
<li><p><strong>critics_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the critic networks.</p></li>
<li><p><strong>ensemble_size</strong> (<em>int</em>) – Number of sub-networks in the critic ensemble.
Default to 10.</p></li>
<li><p><strong>subset_size</strong> (<em>int</em>) – Number of networks in the subset. Default to 2.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – param for soft update of the target network. Default to 0.005.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – discount factor, in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>alpha</strong> (<em>(</em><em>float</em><em>, </em><em>torch.Tensor</em><em>, </em><em>torch.optim.Optimizer</em><em>) or </em><em>float</em>) – entropy
regularization coefficient. Default to 0.2.
If a tuple (target_entropy, log_alpha, alpha_optim) is provided, then
alpha is automatically tuned.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>actor_delay</strong> (<em>int</em>) – Number of critic updates before an actor update.
Default to 20.</p></li>
<li><p><strong>exploration_noise</strong> (<a class="reference internal" href="tianshou.exploration.html#tianshou.exploration.BaseNoise" title="tianshou.exploration.BaseNoise"><em>BaseNoise</em></a>) – add a noise to action for exploration.
Default to None. This is useful when solving hard-exploration problem.</p></li>
<li><p><strong>deterministic_eval</strong> (<em>bool</em>) – whether to use deterministic action (mean
of Gaussian policy) instead of stochastic action sampled by the policy.
Default to True.</p></li>
<li><p><strong>target_mode</strong> (<em>str</em>) – methods to integrate critic values in the subset,
currently support minimum and average. Default to min.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action) or empty string for no bounding.
Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.REDQPolicy.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#tianshou.policy.REDQPolicy" title="tianshou.policy.modelfree.redq.REDQPolicy"><span class="pre">REDQPolicy</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/redq.html#REDQPolicy.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.REDQPolicy.train" title="Link to this definition">¶</a></dt>
<dd><p>Set the module in training mode, except for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.REDQPolicy.sync_weight">
<span class="sig-name descname"><span class="pre">sync_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/redq.html#REDQPolicy.sync_weight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.REDQPolicy.sync_weight" title="Link to this definition">¶</a></dt>
<dd><p>Soft-update the weight for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.REDQPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'obs'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/redq.html#REDQPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.REDQPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which has 2 keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> the hidden state.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.REDQPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/redq.html#REDQPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.REDQPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteSACPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">DiscreteSACPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optimizer</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/discrete_sac.html#DiscreteSACPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteSACPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.SACPolicy" title="tianshou.policy.modelfree.sac.SACPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">SACPolicy</span></code></a></p>
<p>Implementation of SAC for Discrete Action Settings. arXiv:1910.07207.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>actor_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor network.</p></li>
<li><p><strong>critic1</strong> (<em>torch.nn.Module</em>) – the first critic network. (s -&gt; Q(s))</p></li>
<li><p><strong>critic1_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the first
critic network.</p></li>
<li><p><strong>critic2</strong> (<em>torch.nn.Module</em>) – the second critic network. (s -&gt; Q(s))</p></li>
<li><p><strong>critic2_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the second
critic network.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – param for soft update of the target network. Default to 0.005.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – discount factor, in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>alpha</strong> (<em>(</em><em>float</em><em>, </em><em>torch.Tensor</em><em>, </em><em>torch.optim.Optimizer</em><em>) or </em><em>float</em>) – entropy
regularization coefficient. Default to 0.2.
If a tuple (target_entropy, log_alpha, alpha_optim) is provided, the
alpha is automatically tuned.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteSACPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'obs'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/discrete_sac.html#DiscreteSACPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteSACPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which has 2 keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> the hidden state.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteSACPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/discrete_sac.html#DiscreteSACPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteSACPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteSACPolicy.exploration_noise">
<span class="sig-name descname"><span class="pre">exploration_noise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelfree/discrete_sac.html#DiscreteSACPolicy.exploration_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteSACPolicy.exploration_noise" title="Link to this definition">¶</a></dt>
<dd><p>Modify the action from policy.forward with exploration noise.</p>
<p>NOTE: currently does not add any noise! Needs to be overridden by subclasses
to actually do something.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>act</strong> – a data batch or numpy.ndarray which is the action taken by
policy.forward.</p></li>
<li><p><strong>batch</strong> – the input batch for policy.forward, kept for advanced usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>action in the same form of input “act” but with added exploration
noise.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
<section id="imitation">
<h2>Imitation<a class="headerlink" href="#imitation" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.ImitationPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">ImitationPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/imitation/base.html#ImitationPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ImitationPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a></p>
<p>Implementation of vanilla imitation learning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; a)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – for optimizing the model.</p></li>
<li><p><strong>action_space</strong> (<em>gym.Space</em>) – env’s action space.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.ImitationPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ModelOutputBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/base.html#ImitationPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ImitationPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which MUST have the following keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> an numpy.ndarray or a torch.Tensor, the action over                 given batch data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> a dict, an numpy.ndarray or a torch.Tensor, the                 internal state of the policy, <code class="docutils literal notranslate"><span class="pre">None</span></code> as default.</p></li>
</ul>
</p>
</dd>
</dl>
<p>Other keys are user-defined. It depends on the algorithm. For example,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># some code</span>
<span class="k">return</span> <span class="n">Batch</span><span class="p">(</span><span class="n">logits</span><span class="o">=...</span><span class="p">,</span> <span class="n">act</span><span class="o">=...</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dist</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<p>The keyword <code class="docutils literal notranslate"><span class="pre">policy</span></code> is reserved and the corresponding data will be
stored into the replay buffer. For instance,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># some code</span>
<span class="k">return</span> <span class="n">Batch</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">Batch</span><span class="p">(</span><span class="n">log_prob</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">act</span><span class="p">)))</span>
<span class="c1"># and in the sampled data batch, you can directly use</span>
<span class="c1"># batch.policy.log_prob to get your data.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In continuous action space, you should do another step “map_action” to get
the real action:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">act</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">act</span>  <span class="c1"># doesn&#39;t map to the target action range</span>
<span class="n">act</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">map_action</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
</pre></div>
</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.ImitationPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">ags</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/base.html#ImitationPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ImitationPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.BCQPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">BCQPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vae</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.continuous.VAE" title="tianshou.utils.net.continuous.VAE"><span class="pre">VAE</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">vae_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">device</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lmbda</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.75</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_sampled_times</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_sampled_action</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/imitation/bcq.html#BCQPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BCQPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a></p>
<p>Implementation of BCQ algorithm. arXiv:1812.02900.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.continuous.Perturbation" title="tianshou.utils.net.continuous.Perturbation"><em>Perturbation</em></a>) – the actor perturbation. (s, a -&gt; perturbed a)</p></li>
<li><p><strong>actor_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor network.</p></li>
<li><p><strong>critic1</strong> (<em>torch.nn.Module</em>) – the first critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic1_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the first
critic network.</p></li>
<li><p><strong>critic2</strong> (<em>torch.nn.Module</em>) – the second critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic2_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the second
critic network.</p></li>
<li><p><strong>vae</strong> (<a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.continuous.VAE" title="tianshou.utils.net.continuous.VAE"><em>VAE</em></a>) – the VAE network, generating actions similar
to those in batch. (s, a -&gt; generated a)</p></li>
<li><p><strong>vae_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the VAE network.</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>torch.device</em><em>]</em>) – which device to create this model on.
Default to “cpu”.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – discount factor, in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – param for soft update of the target network.
Default to 0.005.</p></li>
<li><p><strong>lmbda</strong> (<em>float</em>) – param for Clipped Double Q-learning. Default to 0.75.</p></li>
<li><p><strong>forward_sampled_times</strong> (<em>int</em>) – the number of sampled actions in forward
function. The policy samples many actions and takes the action with the
max value. Default to 100.</p></li>
<li><p><strong>num_sampled_action</strong> (<em>int</em>) – the number of sampled actions in calculating
target Q. The algorithm samples several actions using VAE, and perturbs
each action to get the target Q. Default to 10.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BCQPolicy.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#tianshou.policy.BCQPolicy" title="tianshou.policy.imitation.bcq.BCQPolicy"><span class="pre">BCQPolicy</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/bcq.html#BCQPolicy.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BCQPolicy.train" title="Link to this definition">¶</a></dt>
<dd><p>Set the module in training mode, except for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BCQPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/bcq.html#BCQPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BCQPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BCQPolicy.sync_weight">
<span class="sig-name descname"><span class="pre">sync_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/bcq.html#BCQPolicy.sync_weight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BCQPolicy.sync_weight" title="Link to this definition">¶</a></dt>
<dd><p>Soft-update the weight for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.BCQPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/bcq.html#BCQPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.BCQPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.CQLPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">CQLPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.continuous.ActorProb" title="tianshou.utils.net.continuous.ActorProb"><span class="pre">ActorProb</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cql_alpha_lr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cql_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optimizer</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_lagrange</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lagrange_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_action</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_action</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_repeat_actions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_min</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_max</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1000000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">device</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'cpu'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/imitation/cql.html#CQLPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.CQLPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.SACPolicy" title="tianshou.policy.modelfree.sac.SACPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">SACPolicy</span></code></a></p>
<p>Implementation of CQL algorithm. arXiv:2006.04779.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.continuous.ActorProb" title="tianshou.utils.net.continuous.ActorProb"><em>ActorProb</em></a>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; a)</p></li>
<li><p><strong>actor_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor network.</p></li>
<li><p><strong>critic1</strong> (<em>torch.nn.Module</em>) – the first critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic1_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the first
critic network.</p></li>
<li><p><strong>critic2</strong> (<em>torch.nn.Module</em>) – the second critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic2_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the second
critic network.</p></li>
<li><p><strong>cql_alpha_lr</strong> (<em>float</em>) – the learning rate of cql_log_alpha. Default to 1e-4.</p></li>
<li><p><strong>cql_weight</strong> (<em>float</em>) – the value of alpha. Default to 1.0.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – param for soft update of the target network.
Default to 0.005.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – discount factor, in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>alpha</strong> (<em>(</em><em>float</em><em>, </em><em>torch.Tensor</em><em>, </em><em>torch.optim.Optimizer</em><em>) or </em><em>float</em>) – entropy
regularization coefficient. Default to 0.2.
If a tuple (target_entropy, log_alpha, alpha_optim) is provided, then
alpha is automatically tuned.</p></li>
<li><p><strong>temperature</strong> (<em>float</em>) – the value of temperature. Default to 1.0.</p></li>
<li><p><strong>with_lagrange</strong> (<em>bool</em>) – whether to use Lagrange. Default to True.</p></li>
<li><p><strong>lagrange_threshold</strong> (<em>float</em>) – the value of tau in CQL(Lagrange).
Default to 10.0.</p></li>
<li><p><strong>min_action</strong> (<em>float</em>) – The minimum value of each dimension of action.
Default to -1.0.</p></li>
<li><p><strong>max_action</strong> (<em>float</em>) – The maximum value of each dimension of action.
Default to 1.0.</p></li>
<li><p><strong>num_repeat_actions</strong> (<em>int</em>) – The number of times the action is repeated
when calculating log-sum-exp. Default to 10.</p></li>
<li><p><strong>alpha_min</strong> (<em>float</em>) – lower bound for clipping cql_alpha. Default to 0.0.</p></li>
<li><p><strong>alpha_max</strong> (<em>float</em>) – upper bound for clipping cql_alpha. Default to 1e6.</p></li>
<li><p><strong>clip_grad</strong> (<em>float</em>) – clip_grad for updating critic network. Default to 1.0.</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>torch.device</em><em>]</em>) – which device to create this model on.
Default to “cpu”.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.CQLPolicy.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#tianshou.policy.CQLPolicy" title="tianshou.policy.imitation.cql.CQLPolicy"><span class="pre">CQLPolicy</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/cql.html#CQLPolicy.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.CQLPolicy.train" title="Link to this definition">¶</a></dt>
<dd><p>Set the module in training mode, except for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.CQLPolicy.sync_weight">
<span class="sig-name descname"><span class="pre">sync_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/cql.html#CQLPolicy.sync_weight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.CQLPolicy.sync_weight" title="Link to this definition">¶</a></dt>
<dd><p>Soft-update the weight for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.CQLPolicy.actor_pred">
<span class="sig-name descname"><span class="pre">actor_pred</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/cql.html#CQLPolicy.actor_pred"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.CQLPolicy.actor_pred" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.CQLPolicy.calc_actor_loss">
<span class="sig-name descname"><span class="pre">calc_actor_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/cql.html#CQLPolicy.calc_actor_loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.CQLPolicy.calc_actor_loss" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.CQLPolicy.calc_pi_values">
<span class="sig-name descname"><span class="pre">calc_pi_values</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs_pi</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obs_to_pred</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/cql.html#CQLPolicy.calc_pi_values"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.CQLPolicy.calc_pi_values" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.CQLPolicy.calc_random_values">
<span class="sig-name descname"><span class="pre">calc_random_values</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/cql.html#CQLPolicy.calc_random_values"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.CQLPolicy.calc_random_values" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.CQLPolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RolloutBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/cql.html#CQLPolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.CQLPolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Pre-process the data from the provided replay buffer.</p>
<p>Used in <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code>. Check out <a class="reference internal" href="../tutorials/concepts.html#process-fn"><span class="std std-ref">policy.process_fn</span></a> for more information.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.CQLPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/cql.html#CQLPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.CQLPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.TD3BCPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">TD3BCPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor:</span> <span class="pre">~torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_optim:</span> <span class="pre">~torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1:</span> <span class="pre">~torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic1_optim:</span> <span class="pre">~torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2:</span> <span class="pre">~torch.nn.modules.module.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic2_optim:</span> <span class="pre">~torch.optim.optimizer.Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tau:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exploration_noise:</span> <span class="pre">~tianshou.exploration.random.BaseNoise</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;tianshou.exploration.random.GaussianNoise</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_noise:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_actor_freq:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_clip:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">2.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs:</span> <span class="pre">~typing.Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/imitation/td3_bc.html#TD3BCPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.TD3BCPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.TD3Policy" title="tianshou.policy.modelfree.td3.TD3Policy"><code class="xref py py-class docutils literal notranslate"><span class="pre">TD3Policy</span></code></a></p>
<p>Implementation of TD3+BC. arXiv:2106.06860.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>actor_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor network.</p></li>
<li><p><strong>critic1</strong> (<em>torch.nn.Module</em>) – the first critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic1_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the first
critic network.</p></li>
<li><p><strong>critic2</strong> (<em>torch.nn.Module</em>) – the second critic network. (s, a -&gt; Q(s, a))</p></li>
<li><p><strong>critic2_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the second
critic network.</p></li>
<li><p><strong>tau</strong> (<em>float</em>) – param for soft update of the target network. Default to 0.005.</p></li>
<li><p><strong>gamma</strong> (<em>float</em>) – discount factor, in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>exploration_noise</strong> (<em>float</em>) – the exploration noise, add to the action.
Default to <code class="docutils literal notranslate"><span class="pre">GaussianNoise(sigma=0.1)</span></code></p></li>
<li><p><strong>policy_noise</strong> (<em>float</em>) – the noise used in updating policy network.
Default to 0.2.</p></li>
<li><p><strong>update_actor_freq</strong> (<em>int</em>) – the update frequency of actor network.
Default to 2.</p></li>
<li><p><strong>noise_clip</strong> (<em>float</em>) – the clipping range used in updating policy network.
Default to 0.5.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – the value of alpha, which controls the weight for TD3 learning
relative to behavior cloning.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action) or empty string for no bounding.
Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.TD3BCPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/td3_bc.html#TD3BCPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.TD3BCPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteBCQPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">DiscreteBCQPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">imitator</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_freq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unlikely_action_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">imitation_logits_penalty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/imitation/discrete_bcq.html#DiscreteBCQPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteBCQPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.DQNPolicy" title="tianshou.policy.modelfree.dqn.DQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DQNPolicy</span></code></a></p>
<p>Implementation of discrete BCQ algorithm. arXiv:1910.01708.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; q_value)</p></li>
<li><p><strong>imitator</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; imitation_logits)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1].</p></li>
<li><p><strong>estimation_step</strong> (<em>int</em>) – the number of steps to look ahead. Default to 1.</p></li>
<li><p><strong>target_update_freq</strong> (<em>int</em>) – the target network update frequency.</p></li>
<li><p><strong>eval_eps</strong> (<em>float</em>) – the epsilon-greedy noise added in evaluation.</p></li>
<li><p><strong>unlikely_action_threshold</strong> (<em>float</em>) – the threshold (tau) for unlikely
actions, as shown in Equ. (17) in the paper. Default to 0.3.</p></li>
<li><p><strong>imitation_logits_penalty</strong> (<em>float</em>) – regularization weight for imitation
logits. Default to 1e-2.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteBCQPolicy.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#tianshou.policy.DiscreteBCQPolicy" title="tianshou.policy.imitation.discrete_bcq.DiscreteBCQPolicy"><span class="pre">DiscreteBCQPolicy</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/discrete_bcq.html#DiscreteBCQPolicy.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteBCQPolicy.train" title="Link to this definition">¶</a></dt>
<dd><p>Set the module in training mode, except for the target network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteBCQPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'obs'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ImitationBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/discrete_bcq.html#DiscreteBCQPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteBCQPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data.</p>
<p>If you need to mask the action, please add a “mask” into batch.obs, for
example, if we have an environment that has “0/1/2” three actions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="o">==</span> <span class="n">Batch</span><span class="p">(</span>
    <span class="n">obs</span><span class="o">=</span><span class="n">Batch</span><span class="p">(</span>
        <span class="n">obs</span><span class="o">=</span><span class="s2">&quot;original obs, with batch_size=1 for demonstration&quot;</span><span class="p">,</span>
        <span class="n">mask</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]]),</span>
        <span class="c1"># action 1 is available</span>
        <span class="c1"># action 0 and 2 are unavailable</span>
    <span class="p">),</span>
    <span class="o">...</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> which has 3 keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">act</span></code> the action.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logits</span></code> the network’s raw output.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">state</span></code> the hidden state.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteBCQPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/discrete_bcq.html#DiscreteBCQPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteBCQPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteCQLPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">DiscreteCQLPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_quantiles</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimation_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_freq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_q_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/imitation/discrete_cql.html#DiscreteCQLPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteCQLPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.QRDQNPolicy" title="tianshou.policy.modelfree.qrdqn.QRDQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">QRDQNPolicy</span></code></a></p>
<p>Implementation of discrete Conservative Q-Learning algorithm. arXiv:2006.04779.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>torch.nn.Module</em>) – a model following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1].</p></li>
<li><p><strong>num_quantiles</strong> (<em>int</em>) – the number of quantile midpoints in the inverse
cumulative distribution function of the value. Default to 200.</p></li>
<li><p><strong>estimation_step</strong> (<em>int</em>) – the number of steps to look ahead. Default to 1.</p></li>
<li><p><strong>target_update_freq</strong> (<em>int</em>) – the target network update frequency (0 if
you do not use the target network).</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>min_q_weight</strong> (<em>float</em>) – the weight for the cql loss.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.QRDQNPolicy" title="tianshou.policy.QRDQNPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">QRDQNPolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteCQLPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/discrete_cql.html#DiscreteCQLPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteCQLPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteCRRPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">DiscreteCRRPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">policy_improvement_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'exp'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ratio_upper_bound</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">20.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_q_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_update_freq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/imitation/discrete_crr.html#DiscreteCRRPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteCRRPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.PGPolicy" title="tianshou.policy.modelfree.pg.PGPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PGPolicy</span></code></a></p>
<p>Implementation of discrete Critic Regularized Regression. arXiv:2006.15134.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>critic</strong> (<em>torch.nn.Module</em>) – the action-value critic (i.e., Q function)
network. (s -&gt; Q(s, *))</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>policy_improvement_mode</strong> (<em>str</em>) – type of the weight function f. Possible
values: “binary”/”exp”/”all”. Default to “exp”.</p></li>
<li><p><strong>ratio_upper_bound</strong> (<em>float</em>) – when policy_improvement_mode is “exp”, the value
of the exp function is upper-bounded by this parameter. Default to 20.</p></li>
<li><p><strong>beta</strong> (<em>float</em>) – when policy_improvement_mode is “exp”, this is the denominator
of the exp function. Default to 1.</p></li>
<li><p><strong>min_q_weight</strong> (<em>float</em>) – weight for CQL loss/regularizer. Default to 10.</p></li>
<li><p><strong>target_update_freq</strong> (<em>int</em>) – the target network update frequency (0 if
you do not use the target network). Default to 0.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize the reward to Normal(0, 1).
Default to False.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.PGPolicy" title="tianshou.policy.PGPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PGPolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteCRRPolicy.sync_weight">
<span class="sig-name descname"><span class="pre">sync_weight</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/discrete_crr.html#DiscreteCRRPolicy.sync_weight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteCRRPolicy.sync_weight" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.DiscreteCRRPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/discrete_crr.html#DiscreteCRRPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.DiscreteCRRPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.GAILPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">GAILPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">critic</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Distribution</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expert_buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">disc_net</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">disc_optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">disc_update_num</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps_clip</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual_clip</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_clip</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">advantage_normalization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recompute_advantage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/imitation/gail.html#GAILPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.GAILPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.PPOPolicy" title="tianshou.policy.modelfree.ppo.PPOPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PPOPolicy</span></code></a></p>
<p>Implementation of Generative Adversarial Imitation Learning. arXiv:1606.03476.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>actor</strong> (<em>torch.nn.Module</em>) – the actor network following the rules in
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. (s -&gt; logits)</p></li>
<li><p><strong>critic</strong> (<em>torch.nn.Module</em>) – the critic network. (s -&gt; V(s))</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for actor and critic network.</p></li>
<li><p><strong>dist_fn</strong> – distribution class for computing the action.</p></li>
<li><p><strong>expert_buffer</strong> (<a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.ReplayBuffer"><em>ReplayBuffer</em></a>) – the replay buffer contains expert experience.</p></li>
<li><p><strong>disc_net</strong> (<em>torch.nn.Module</em>) – the discriminator network with input dim equals
state dim plus action dim and output dim equals 1.</p></li>
<li><p><strong>disc_optim</strong> (<em>torch.optim.Optimizer</em>) – the optimizer for the discriminator
network.</p></li>
<li><p><strong>disc_update_num</strong> (<em>int</em>) – the number of discriminator grad steps per model grad
step. Default to 4.</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1]. Default to 0.99.</p></li>
<li><p><strong>eps_clip</strong> (<em>float</em>) – <span class="math notranslate nohighlight">\(\epsilon\)</span> in <span class="math notranslate nohighlight">\(L_{CLIP}\)</span> in the original
paper. Default to 0.2.</p></li>
<li><p><strong>dual_clip</strong> (<em>float</em>) – a parameter c mentioned in arXiv:1912.09729 Equ. 5,
where c &gt; 1 is a constant indicating the lower bound.
Default to 5.0 (set None if you do not want to use it).</p></li>
<li><p><strong>value_clip</strong> (<em>bool</em>) – a parameter mentioned in arXiv:1811.02553 Sec. 4.1.
Default to True.</p></li>
<li><p><strong>advantage_normalization</strong> (<em>bool</em>) – whether to do per mini-batch advantage
normalization. Default to True.</p></li>
<li><p><strong>recompute_advantage</strong> (<em>bool</em>) – whether to recompute advantage every update
repeat according to <a class="reference external" href="https://arxiv.org/pdf/2006.05990.pdf">https://arxiv.org/pdf/2006.05990.pdf</a> Sec. 3.5.
Default to False.</p></li>
<li><p><strong>vf_coef</strong> (<em>float</em>) – weight for value loss. Default to 0.5.</p></li>
<li><p><strong>ent_coef</strong> (<em>float</em>) – weight for entropy loss. Default to 0.01.</p></li>
<li><p><strong>max_grad_norm</strong> (<em>float</em>) – clipping gradients in back propagation. Default to
None.</p></li>
<li><p><strong>gae_lambda</strong> (<em>float</em>) – in [0, 1], param for Generalized Advantage Estimation.
Default to 0.95.</p></li>
<li><p><strong>reward_normalization</strong> (<em>bool</em>) – normalize estimated values to have std close
to 1, also normalize the advantage to Normal(0, 1). Default to False.</p></li>
<li><p><strong>max_batchsize</strong> (<em>int</em>) – the maximum size of the batch when computing GAE,
depends on the size of available memory and the memory cost of the model;
should be as large as possible within the memory constraint. Default to 256.</p></li>
<li><p><strong>action_scaling</strong> (<em>bool</em>) – whether to map actions from range [-1, 1] to range
[action_spaces.low, action_spaces.high]. Default to True.</p></li>
<li><p><strong>action_bound_method</strong> (<em>str</em>) – method to bound action to range [-1, 1], can be
either “clip” (for simply clipping the action), “tanh” (for applying tanh
squashing) for now, or empty string for no bounding. Default to “clip”.</p></li>
<li><p><strong>action_space</strong> (<em>Optional</em><em>[</em><em>gym.Space</em><em>]</em>) – env’s action space, mandatory if you want
to use option “action_scaling” or “action_bound_method”. Default to None.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
<li><p><strong>deterministic_eval</strong> (<em>bool</em>) – whether to use deterministic action instead of
stochastic action sampled by the policy. Default to False.</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.PPOPolicy" title="tianshou.policy.PPOPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">PPOPolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.GAILPolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LogpOldProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/gail.html#GAILPolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.GAILPolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Pre-process the data from the provided replay buffer.</p>
<p>Used in <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code>. Check out <a class="reference internal" href="../tutorials/concepts.html#process-fn"><span class="std std-ref">policy.process_fn</span></a> for more information.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.GAILPolicy.disc">
<span class="sig-name descname"><span class="pre">disc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/gail.html#GAILPolicy.disc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.GAILPolicy.disc" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.GAILPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repeat</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/imitation/gail.html#GAILPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.GAILPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="model-based">
<h2>Model-based<a class="headerlink" href="#model-based" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.PSRLPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">PSRLPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trans_count_prior</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rew_mean_prior</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rew_std_prior</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_done_loop</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/psrl.html#PSRLPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.PSRLPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a></p>
<p>Implementation of Posterior Sampling Reinforcement Learning.</p>
<p>Reference: Strens M. A Bayesian framework for reinforcement learning [C]
//ICML. 2000, 2000: 943-950.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trans_count_prior</strong> (<em>np.ndarray</em>) – dirichlet prior (alphas), with shape
(n_state, n_action, n_state).</p></li>
<li><p><strong>rew_mean_prior</strong> (<em>np.ndarray</em>) – means of the normal priors of rewards,
with shape (n_state, n_action).</p></li>
<li><p><strong>rew_std_prior</strong> (<em>np.ndarray</em>) – standard deviations of the normal priors
of rewards, with shape (n_state, n_action).</p></li>
<li><p><strong>discount_factor</strong> (<em>float</em>) – in [0, 1].</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) – for precision control in value iteration.</p></li>
<li><p><strong>add_done_loop</strong> (<em>bool</em>) – whether to add an extra self-loop for the
terminal state in MDP. Default to False.</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.PSRLPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ActBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/psrl.html#PSRLPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.PSRLPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data with PSRL model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.Batch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Batch</span></code></a> with “act” key containing
the action.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.PSRLPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/psrl.html#PSRLPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.PSRLPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.ICMPolicy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">ICMPolicy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">policy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><span class="pre">BasePolicy</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.discrete.IntrinsicCuriosityModule" title="tianshou.utils.net.discrete.IntrinsicCuriosityModule"><span class="pre">IntrinsicCuriosityModule</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">optim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reward_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_loss_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/icm.html#ICMPolicy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ICMPolicy" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a></p>
<p>Implementation of Intrinsic Curiosity Module. arXiv:1705.05363.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>policy</strong> (<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><em>BasePolicy</em></a>) – a base policy to add ICM to.</p></li>
<li><p><strong>model</strong> (<a class="reference internal" href="tianshou.utils.html#tianshou.utils.net.discrete.IntrinsicCuriosityModule" title="tianshou.utils.net.discrete.IntrinsicCuriosityModule"><em>IntrinsicCuriosityModule</em></a>) – the ICM model.</p></li>
<li><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – a torch.optim for optimizing the model.</p></li>
<li><p><strong>lr_scale</strong> (<em>float</em>) – the scaling factor for ICM learning.</p></li>
<li><p><strong>forward_loss_weight</strong> (<em>float</em>) – the weight for forward model loss.</p></li>
<li><p><strong>lr_scheduler</strong> – a learning rate scheduler that adjusts the learning rate in
optimizer in each policy.update(). Default to None (no lr_scheduler).</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a> for more detailed
explanation.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.ICMPolicy.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#tianshou.policy.ICMPolicy" title="tianshou.policy.modelbased.icm.ICMPolicy"><span class="pre">ICMPolicy</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/icm.html#ICMPolicy.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ICMPolicy.train" title="Link to this definition">¶</a></dt>
<dd><p>Set the module in training mode.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.ICMPolicy.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/icm.html#ICMPolicy.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ICMPolicy.forward" title="Link to this definition">¶</a></dt>
<dd><p>Compute action over the given batch data by inner policy.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>Please refer to <a class="reference internal" href="#tianshou.policy.BasePolicy.forward" title="tianshou.policy.BasePolicy.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> for
more detailed explanation.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.ICMPolicy.exploration_noise">
<span class="sig-name descname"><span class="pre">exploration_noise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/icm.html#ICMPolicy.exploration_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ICMPolicy.exploration_noise" title="Link to this definition">¶</a></dt>
<dd><p>Modify the action from policy.forward with exploration noise.</p>
<p>NOTE: currently does not add any noise! Needs to be overridden by subclasses
to actually do something.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>act</strong> – a data batch or numpy.ndarray which is the action taken by
policy.forward.</p></li>
<li><p><strong>batch</strong> – the input batch for policy.forward, kept for advanced usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>action in the same form of input “act” but with added exploration
noise.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.ICMPolicy.set_eps">
<span class="sig-name descname"><span class="pre">set_eps</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/icm.html#ICMPolicy.set_eps"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ICMPolicy.set_eps" title="Link to this definition">¶</a></dt>
<dd><p>Set the eps for epsilon-greedy exploration.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.ICMPolicy.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RolloutBatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/icm.html#ICMPolicy.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ICMPolicy.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Pre-process the data from the provided replay buffer.</p>
<p>Used in <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code>. Check out <a class="reference internal" href="../tutorials/concepts.html#process-fn"><span class="std std-ref">policy.process_fn</span></a> for more information.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.ICMPolicy.post_process_fn">
<span class="sig-name descname"><span class="pre">post_process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">BatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/icm.html#ICMPolicy.post_process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ICMPolicy.post_process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Post-process the data from the provided replay buffer.</p>
<p>Typical usage is to update the sampling weight in prioritized
experience replay. Used in <code class="xref py py-meth docutils literal notranslate"><span class="pre">update()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.ICMPolicy.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/modelbased/icm.html#ICMPolicy.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.ICMPolicy.learn" title="Link to this definition">¶</a></dt>
<dd><p>Update policy with a given batch of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dict, including the data needed to be logged (e.g., loss).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to distinguish the collecting state, updating state and
testing state, you can check the policy state by <code class="docutils literal notranslate"><span class="pre">self.training</span></code>
and <code class="docutils literal notranslate"><span class="pre">self.updating</span></code>. Please refer to <a class="reference internal" href="../tutorials/concepts.html#policy-state"><span class="std std-ref">States for policy</span></a> for more
detailed explanation.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.distributions.Normal</span></code> and
<code class="docutils literal notranslate"><span class="pre">torch.distributions.Categorical</span></code> to calculate the log_prob,
please be careful about the shape: Categorical distribution gives
“[batch_size]” shape while Normal distribution gives “[batch_size,
1]” shape. The auto-broadcasting of numerical operation with torch
tensors will amplify this error.</p>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="multi-agent">
<h2>Multi-agent<a class="headerlink" href="#multi-agent" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="tianshou.policy.MultiAgentPolicyManager">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tianshou.policy.</span></span><span class="sig-name descname"><span class="pre">MultiAgentPolicyManager</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">policies</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><span class="pre">BasePolicy</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">env</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.env.html#tianshou.env.PettingZooEnv" title="tianshou.env.pettingzoo_env.PettingZooEnv"><span class="pre">PettingZooEnv</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/tianshou/policy/multiagent/mapolicy.html#MultiAgentPolicyManager"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.MultiAgentPolicyManager" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a></p>
<p>Multi-agent policy manager for MARL.</p>
<p>This multi-agent policy manager accepts a list of
<a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.BasePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">BasePolicy</span></code></a>. It dispatches the batch data to each
of these policies when the “forward” is called. The same as “process_fn”
and “learn”: it splits the data and feeds them to each policy. A figure in
<a class="reference internal" href="../tutorials/cheatsheet.html#marl-example"><span class="std std-ref">Multi-Agent Reinforcement Learning</span></a> can help you better understand this procedure.</p>
<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.MultiAgentPolicyManager.replace_policy">
<span class="sig-name descname"><span class="pre">replace_policy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">policy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#tianshou.policy.BasePolicy" title="tianshou.policy.base.BasePolicy"><span class="pre">BasePolicy</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">agent_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/multiagent/mapolicy.html#MultiAgentPolicyManager.replace_policy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.MultiAgentPolicyManager.replace_policy" title="Link to this definition">¶</a></dt>
<dd><p>Replace the “agent_id”th policy in this manager.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.MultiAgentPolicyManager.process_fn">
<span class="sig-name descname"><span class="pre">process_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">buffer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.ReplayBuffer" title="tianshou.data.buffer.base.ReplayBuffer"><span class="pre">ReplayBuffer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">indice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/multiagent/mapolicy.html#MultiAgentPolicyManager.process_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.MultiAgentPolicyManager.process_fn" title="Link to this definition">¶</a></dt>
<dd><p>Dispatch batch data from obs.agent_id to every policy’s process_fn.</p>
<p>Save original multi-dimensional rew in “save_rew”, set rew to the
reward of each agent during their “process_fn”, and restore the
original reward afterwards.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.MultiAgentPolicyManager.exploration_noise">
<span class="sig-name descname"><span class="pre">exploration_noise</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">BatchProtocol</span></span></span><a class="reference internal" href="../_modules/tianshou/policy/multiagent/mapolicy.html#MultiAgentPolicyManager.exploration_noise"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.MultiAgentPolicyManager.exploration_noise" title="Link to this definition">¶</a></dt>
<dd><p>Add exploration noise from sub-policy onto act.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.MultiAgentPolicyManager.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="tianshou.data.html#tianshou.data.Batch" title="tianshou.data.batch.Batch"><span class="pre">Batch</span></a></span></span><a class="reference internal" href="../_modules/tianshou/policy/multiagent/mapolicy.html#MultiAgentPolicyManager.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.MultiAgentPolicyManager.forward" title="Link to this definition">¶</a></dt>
<dd><p>Dispatch batch data from obs.agent_id to every policy’s forward.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state</strong> – if None, it means all agents have no state. If not
None, it should contain keys of “agent_1”, “agent_2”, …</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a Batch with the following contents:</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;act&quot;</span><span class="p">:</span> <span class="n">actions</span> <span class="n">corresponding</span> <span class="n">to</span> <span class="n">the</span> <span class="nb">input</span>
    <span class="s2">&quot;state&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;agent_1&quot;</span><span class="p">:</span> <span class="n">output</span> <span class="n">state</span> <span class="n">of</span> <span class="n">agent_1</span><span class="s1">&#39;s policy for the state</span>
        <span class="s2">&quot;agent_2&quot;</span><span class="p">:</span> <span class="n">xxx</span>
        <span class="o">...</span>
        <span class="s2">&quot;agent_n&quot;</span><span class="p">:</span> <span class="n">xxx</span><span class="p">}</span>
    <span class="s2">&quot;out&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;agent_1&quot;</span><span class="p">:</span> <span class="n">output</span> <span class="n">of</span> <span class="n">agent_1</span><span class="s1">&#39;s policy for the input</span>
        <span class="s2">&quot;agent_2&quot;</span><span class="p">:</span> <span class="n">xxx</span>
        <span class="o">...</span>
        <span class="s2">&quot;agent_n&quot;</span><span class="p">:</span> <span class="n">xxx</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tianshou.policy.MultiAgentPolicyManager.learn">
<span class="sig-name descname"><span class="pre">learn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">RolloutBatchProtocol</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/tianshou/policy/multiagent/mapolicy.html#MultiAgentPolicyManager.learn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tianshou.policy.MultiAgentPolicyManager.learn" title="Link to this definition">¶</a></dt>
<dd><p>Dispatch the data to all policies for learning.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a dict with the following contents:</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;agent_1/item1&quot;</span><span class="p">:</span> <span class="n">item</span> <span class="mi">1</span> <span class="n">of</span> <span class="n">agent_1</span><span class="s1">&#39;s policy.learn output</span>
    <span class="s2">&quot;agent_1/item2&quot;</span><span class="p">:</span> <span class="n">item</span> <span class="mi">2</span> <span class="n">of</span> <span class="n">agent_1</span><span class="s1">&#39;s policy.learn output</span>
    <span class="s2">&quot;agent_2/xxx&quot;</span><span class="p">:</span> <span class="n">xxx</span>
    <span class="o">...</span>
    <span class="s2">&quot;agent_n/xxx&quot;</span><span class="p">:</span> <span class="n">xxx</span>
<span class="p">}</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="tianshou.env.html" class="btn btn-neutral float-left" title="tianshou.env" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tianshou.trainer.html" class="btn btn-neutral float-right" title="tianshou.trainer" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Tianshou contributors..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>